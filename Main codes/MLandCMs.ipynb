{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning (ML) and committee models (CMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import fsspec\n",
    "import netCDF4 as nc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from esmtools.stats import autocorr, corr\n",
    "from scipy.signal import correlate2d\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindcast_observation = xr.open_zarr('D:\\Downloads\\s2s-ai-challenge-template-hydroinformatics-group-master-data\\data\\hindcast-like-observations_2000-2019_biweekly_deterministic.zarr')\n",
    "obs00_19_ter = xr.open_zarr('D:\\Downloads\\s2s-ai-challenge-template-hydroinformatics-group-master-data\\data\\hindcast-like-observations_2000-2019_biweekly_terciled.zarr')\n",
    "tercile_edges = xr.open_dataset('D:\\Downloads\\s2s-ai-challenge-template-hydroinformatics-group-master-data\\data\\hindcast-like-observations_2000-2019_biweekly_tercile-edges.nc')\n",
    "hindcast_ecmwf = xr.open_zarr('D:\\Downloads\\s2s-ai-challenge-template-hydroinformatics-group-master-data\\data\\ecmwf_hindcast-input_2000-2019_biweekly_deterministic.zarr')\n",
    "forecast_ecmwf = xr.open_zarr('D:\\Downloads\\s2s-ai-challenge-template-hydroinformatics-group-master-data\\data\\ecmwf_forecast-input_2020_biweekly_deterministic.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_observation = xr.open_zarr('D:/Downloads/s2s-ai-challenge-template-hydroinformatics-group-master-data/data/forecast-like-observations_2020_biweekly_deterministic.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tercile_forecast = nc.Dataset(\"D:\\\\Downloads\\\\s2s-ai-challenge-template-hydroinformatics-group-master-data\\\\data\\\\forecast-like-observations_2020_biweekly_terciled.nc\")\n",
    "ter_forecast = xr.open_dataset(xr.backends.NetCDF4DataStore(tercile_forecast))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classes(dataset, observation, tercile_edges, variable, lead_time, start, end):\n",
    "    \n",
    "    x = dataset[variable].sel(forecast_time= slice(start, end)).isel(lead_time = lead_time)\n",
    "    \n",
    "    upper = []\n",
    "    lower = []\n",
    "    for i in range(0,len(dataset.forecast_time)):\n",
    "        if i < 53:\n",
    "            u = tercile_edges[variable].isel(week = i, category_edge = 1, lead_time = lead_time).values\n",
    "            upper.append(u)\n",
    "            l = tercile_edges[variable].isel(week = i, category_edge = 0, lead_time = lead_time).values\n",
    "            lower.append(l)\n",
    "\n",
    "    for i in range(0,(int(len(dataset.forecast_time)/53)-1)):\n",
    "        for j in range(0, 53):\n",
    "            upper.append(upper[j])\n",
    "            lower.append(lower[j])\n",
    "            \n",
    "    dataset['upper']=(['forecast_time', 'latitude', 'longitude'], upper)\n",
    "    dataset['lower']=(['forecast_time','latitude', 'longitude'], lower)\n",
    "\n",
    "    u = dataset.upper.sel(forecast_time= slice(start, end))\n",
    "\n",
    "    l = dataset.lower.sel(forecast_time= slice(start, end))\n",
    "    \n",
    "    forecast_time = x.forecast_time\n",
    "    \n",
    "    classes = []\n",
    "    \n",
    "    for i in range(0, len(forecast_time)):\n",
    "\n",
    "        above = (dataset[variable].isel(forecast_time = i) * 0 + 2).where(dataset[variable].isel(forecast_time = i) - dataset.upper.isel(forecast_time = i) > 0, 0)\n",
    "        nnormal = (dataset[variable].isel(forecast_time = i) * 0 + 1).where(dataset[variable].isel(forecast_time = i) - dataset.upper.isel(forecast_time = i) <= 0, 0).where(dataset[variable].isel(forecast_time = i) - dataset.lower.isel(forecast_time = i) >= 0, 0)\n",
    "        below = (dataset[variable].isel(forecast_time = i) * 0 ).where(dataset[variable].isel(forecast_time = i) - dataset.lower.isel(forecast_time = i) < 0, 0)\n",
    "\n",
    "        c = above + nnormal + below\n",
    "#         c = c.where(np.isfinite(observation.tp.isel(forecast_time=i, lead_time=0))==1)\n",
    "\n",
    "        classes.append(c)\n",
    "#         \n",
    "        \n",
    "#     dataset['classes']=(['forecast_time'], classes)\n",
    "#     dataset['classes'] = ([], classes)\n",
    "    combined = xr.concat(classes, dim = 'forecast_time')\n",
    "    cl = combined.where(np.isfinite(observation.tp.isel(forecast_time=0, lead_time=0))==1)\n",
    "#     dataset['cl'] = cl\n",
    "    del dataset['upper']\n",
    "    del dataset['lower']\n",
    "    return cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = create_classes(hindcast_observation, hindcast_observation, tercile_edges, 'tp', 0, '2000-01-01', '2019-12-30')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest neighbors algorithm (K-NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the knn model\n",
    "\n",
    "# in this code the selected data is fixed to the first 18 years for training and the last two years for testing\n",
    "\n",
    "# import the data\n",
    "\n",
    "# hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).chunk(dict(forecast_time=-1))\n",
    "hobs = hindcast_observation.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time  = 0).chunk(dict(forecast_time=-1))\n",
    "hobs_int = hobs.interpolate_na(dim = 'forecast_time')\n",
    "hecm_int = hecm.interpolate_na(dim = 'forecast_time')\n",
    "\n",
    "target_cl = target_class.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).fillna(10).chunk(dict(forecast_time=-1))\n",
    "\n",
    "# modify the longitude and latitude to the selected regions:\n",
    "\n",
    "# regions\n",
    "\n",
    "region1 = hobs.sel(longitude = slice(95, 100)).sel(latitude = slice(36, 31))\n",
    "region2 = hobs.sel(longitude = slice(100, 105)).sel(latitude = slice(60, 55))\n",
    "region3 = hobs.sel(longitude = slice(5, 10)).sel(latitude = slice(53, 48))\n",
    "\n",
    "# longitude = region1.longitude\n",
    "# latitude  = region1.latitude\n",
    "\n",
    "# longitude = region2.longitude\n",
    "# latitude  = region2.latitude\n",
    "\n",
    "longitude = region3.longitude\n",
    "latitude  = region3.latitude\n",
    "\n",
    "# Empty matrices for the results\n",
    "\n",
    "x = hecm_int.mean(dim = 'realization').sel(forecast_time = slice('2018-01-01', '2019-12-30'))\n",
    "\n",
    "prediction_an = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "prediction_nn = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "prediction_bn = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "\n",
    "testing = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "\n",
    "training_acc = np.zeros((1, len(latitude), len(longitude)))\n",
    "testing_acc = np.zeros((1, len(latitude), len(longitude)))\n",
    "\n",
    "i = 0\n",
    "for long in longitude:\n",
    "    \n",
    "    j = 0\n",
    "    for lat in latitude:\n",
    "        \n",
    "        # choose the input and output data\n",
    "        \n",
    "        # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "        hecm_cell = hecm.sel(longitude = long, latitude = lat)\n",
    "        hobs_cell = hobs.sel(longitude = long, latitude = lat)\n",
    "        hobs_int = hobs_cell.interpolate_na(dim = 'forecast_time')\n",
    "        hecm_int = hecm_cell.interpolate_na(dim = 'forecast_time')\n",
    "        # hobs_int['cl'] = target_cl\n",
    "        \n",
    "        # operations\n",
    "        # x inputs\n",
    "        \n",
    "        # training data\n",
    "\n",
    "        x = hobs_int.sel(forecast_time = slice('2000-01-01', '2012-12-30'))\n",
    "        xx = x.to_dataframe().reset_index()\n",
    "        xx_inp = xx.drop(['latitude','longitude', 'lead_time', 'valid_time'], axis = 'columns')\n",
    "        xx_inpVar = xx_inp[['tp','t2m']]\n",
    "        x_train = xx_inpVar.values\n",
    "\n",
    "        y = target_cl.sel(longitude = long, latitude = lat).sel(forecast_time = slice('2000-01-01', '2012-12-30'))\n",
    "        yy = y.to_dataframe().reset_index()\n",
    "        yy_inp = yy.drop(['latitude','longitude', 'lead_time'], axis = 'columns') # valid_time is not found in the case of using target_class\n",
    "        yy_inpVar = yy_inp[['tp']]\n",
    "        y_train = yy_inpVar.values.ravel()\n",
    "\n",
    "        # testing data\n",
    "\n",
    "        x = hecm_int.mean(dim = 'realization').sel(forecast_time = slice('2018-01-01', '2019-12-30'))\n",
    "        xx = x.to_dataframe().reset_index()\n",
    "        xx_inp = xx.drop(['latitude','longitude', 'lead_time', 'valid_time'], axis = 'columns')\n",
    "        xx_inpVar = xx_inp[['tp','t2m']]\n",
    "        x_test = xx_inpVar.values\n",
    "\n",
    "        y = target_cl.sel(longitude = long, latitude = lat).sel(forecast_time = slice('2018-01-01', '2019-12-30'))\n",
    "        yy = y.to_dataframe().reset_index()\n",
    "        yy_inp = yy.drop(['latitude','longitude', 'lead_time'], axis = 'columns') # valid_time is not found in the case of using target_class\n",
    "        yy_inpVar = yy_inp[['tp']]\n",
    "        y_test = yy_inpVar.values.ravel()\n",
    "\n",
    "        # Building the Knn model\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors= 3)\n",
    "        \n",
    "        knn_model = knn.fit(x_train, y_train)\n",
    "\n",
    "        training_accuracy = knn_model.score(x_train, y_train)\n",
    "        \n",
    "        # Knn predictions\n",
    "\n",
    "        yy_pred = knn.predict_proba(x_test)\n",
    "        accuracy = knn_model.score(x_test, y_test)\n",
    "        \n",
    "        # add the values to the empty array\n",
    "        \n",
    "        prediction_an[:, j, i] = yy_pred[:,2]\n",
    "        prediction_nn[:, j, i] = yy_pred[:,1]\n",
    "        prediction_bn[:, j, i] = yy_pred[:,0]\n",
    "        \n",
    "        testing[:, j, i] = y_test\n",
    "        \n",
    "        training_acc[0, j, i] = training_accuracy \n",
    "        testing_acc[0, j, i] = accuracy   \n",
    "        \n",
    "        j = j + 1 \n",
    "    \n",
    "        print(1)\n",
    "        \n",
    "    i = i + 1\n",
    "\n",
    "# Make an xarray of predictions data\n",
    "\n",
    "time_step = [i for i in range(0, len(yy_pred))]\n",
    "\n",
    "longitude = longitude.values\n",
    "latitude = latitude.values\n",
    "\n",
    "predict_an = xr.DataArray(\n",
    "    data=prediction_an,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "        longitude=([\"longitude\"], longitude),\n",
    "        latitude=([\"latitude\"], latitude),      \n",
    "        time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "        description=\"predictions_an\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "predict_nn = xr.DataArray(\n",
    "    data=prediction_nn,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "        longitude=([\"longitude\"], longitude),\n",
    "        latitude=([\"latitude\"], latitude),      \n",
    "        time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "        description=\"predictions_nn\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "predict_bn = xr.DataArray(\n",
    "    data=prediction_bn,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "        longitude=([\"longitude\"], longitude),\n",
    "        latitude=([\"latitude\"], latitude),      \n",
    "        time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "        description=\"predictions_bn\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Make an xarray of tests data\n",
    "\n",
    "test = xr.DataArray(\n",
    "    data=testing,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "        longitude=([\"longitude\"], longitude),\n",
    "        latitude=([\"latitude\"], latitude),       \n",
    "        time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "        description=\"predictions\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# make categories from the predictions\n",
    "\n",
    "categories = xr.concat([predict_bn, predict_nn, predict_an],'category').assign_coords(category=['below normal', 'near normal', 'above normal'])\n",
    "\n",
    "categorical_p = categories.rename({'time_step':'forecast_time'})\n",
    "\n",
    "# Make an xarray for the testing accuracies\n",
    "\n",
    "test_acc = xr.DataArray(\n",
    "    data=testing_acc,\n",
    "    dims=[\"training_accuracy\", \"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "        longitude=([\"longitude\"], longitude),\n",
    "        latitude=([\"latitude\"], latitude),       \n",
    "    ),\n",
    "    attrs=dict(\n",
    "        description=\"testing accuracy\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Make an xarray for the training accuracies\n",
    "\n",
    "train_acc = xr.DataArray(\n",
    "    data=training_acc,\n",
    "    dims=[\"training_accuracy\", \"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "        longitude=([\"longitude\"], longitude),\n",
    "        latitude=([\"latitude\"], latitude),       \n",
    "    ),\n",
    "    attrs=dict(\n",
    "        description=\"testing accuracy\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindcast_observation = xr.open_zarr('D:\\Downloads\\s2s-ai-challenge-template-hydroinformatics-group-master-data\\data\\hindcast-like-observations_2000-2019_biweekly_deterministic.zarr')\n",
    "hindcast_ecmwf = xr.open_zarr('D:\\Downloads\\s2s-ai-challenge-template-hydroinformatics-group-master-data\\data\\ecmwf_hindcast-input_2000-2019_biweekly_deterministic.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the knn model\n",
    "\n",
    "# in this code the selected data is fixed to the first 18 years for training and the last two years for testing\n",
    "\n",
    "# adding the spatial term\n",
    "\n",
    "# import the data\n",
    "\n",
    "# hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).chunk(dict(forecast_time=-1))\n",
    "hobs = hindcast_observation.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time  = 0).chunk(dict(forecast_time=-1))\n",
    "hobs_int = hobs.interpolate_na(dim = 'forecast_time')\n",
    "hecm_int = hecm.interpolate_na(dim = 'forecast_time')\n",
    "\n",
    "target_cl = target_class.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).fillna(10).chunk(dict(forecast_time=-1))\n",
    "\n",
    "# modify the longitude and latitude to the selected regions:\n",
    "\n",
    "# regions\n",
    "\n",
    "region1 = hobs.sel(longitude = slice(95, 100)).sel(latitude = slice(36, 31))\n",
    "region2 = hobs.sel(longitude = slice(100, 105)).sel(latitude = slice(60, 55))\n",
    "region3 = hobs.sel(longitude = slice(5, 10)).sel(latitude = slice(53, 48))\n",
    "\n",
    "# longitude = region1.longitude\n",
    "# latitude  = region1.latitude\n",
    "\n",
    "# longitude = region2.longitude\n",
    "# latitude  = region2.latitude\n",
    "\n",
    "longitude = region3.longitude\n",
    "latitude  = region3.latitude\n",
    "\n",
    "# Empty matrices for the results\n",
    "\n",
    "x = hecm_int.mean(dim = 'realization').sel(forecast_time = slice('2018-01-01', '2019-12-30'))\n",
    "\n",
    "prediction_an = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "prediction_nn = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "prediction_bn = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "\n",
    "testing = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "\n",
    "training_acc = np.zeros((1, len(latitude), len(longitude)))\n",
    "testing_acc = np.zeros((1, len(latitude), len(longitude)))\n",
    "\n",
    "i = 0\n",
    "for long in longitude:\n",
    "    \n",
    "    j = 0\n",
    "    for lat in latitude:\n",
    "        \n",
    "        # choose the input and output data\n",
    "        \n",
    "        # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "        hecm_cell = hecm.sel(longitude = long, latitude = lat)\n",
    "        hobs_cell = hobs.sel(longitude = long, latitude = lat)\n",
    "        hobs_int = hobs_cell.interpolate_na(dim = 'forecast_time')\n",
    "        hecm_int = hecm_cell.interpolate_na(dim = 'forecast_time')\n",
    "        # hobs_int['cl'] = target_cl\n",
    "        \n",
    "        # operations\n",
    "        # x inputs\n",
    "        \n",
    "        # training data\n",
    "        x = hobs_int.sel(forecast_time = slice('2000-01-01', '2012-12-30'))\n",
    "        xx = x.to_dataframe().reset_index()\n",
    "        \n",
    "        # spatially correlated inputs\n",
    "#         spatial = 'one'\n",
    "#         spatial = 'two'\n",
    "        spatial = 'three'\n",
    "\n",
    "        # add the correlated inputs\n",
    "        xx['tp_lag1'] = xx.tp.shift(1).bfill()\n",
    "\n",
    "        if spatial == 'one':\n",
    "            disla = lat - (1 * 1.5)\n",
    "            dislo = long - (1 * 1.5)\n",
    "            dla = np.arange(0, 4.5, 1.5)\n",
    "            dlo = np.arange(0, 4.5, 1.5)\n",
    "            v = 'var 4'\n",
    "\n",
    "        if spatial == 'two':\n",
    "            disla = lat - (2 * 1.5)\n",
    "            dislo = long - (2 * 1.5)\n",
    "            dla = np.arange(0, 7.5, 1.5)\n",
    "            dlo = np.arange(0, 7.5, 1.5)\n",
    "            v = 'var 12'\n",
    "\n",
    "        if spatial == 'three':\n",
    "            disla = lat - (3 * 1.5)\n",
    "            dislo = long - (3 * 1.5)\n",
    "            dla = np.arange(0, 10.5, 1.5)\n",
    "            dlo = np.arange(0, 10.5, 1.5)\n",
    "            v = 'var 24'\n",
    "\n",
    "        # var = []\n",
    "        nam = 0\n",
    "        for ii in dlo:\n",
    "            for jj in dla:\n",
    "\n",
    "                xx['Var'] = hobs.tp.sel(latitude = disla + jj, longitude = dislo + ii).sel(forecast_time = slice('2000-01-01', '2012-12-30'))\n",
    "                name = 'var {}'.format(nam)\n",
    "                xx.rename(columns={'Var': name}, inplace=True)\n",
    "                nam = nam + 1\n",
    "\n",
    "        x_dropna = xx.dropna(how='all', axis=1)\n",
    "        x_inp = x_dropna.drop(['forecast_time','latitude','longitude', 'lead_time', 'valid_time'], axis = 'columns')\n",
    "        x_inpVar = x_inp.drop([v], axis = 'columns')\n",
    "\n",
    "        x_train = x_inpVar.values\n",
    "\n",
    "        y = target_cl.sel(longitude = long, latitude = lat).sel(forecast_time = slice('2000-01-01', '2012-12-30'))\n",
    "        yy = y.to_dataframe().reset_index()\n",
    "        yy_inp = yy.drop(['latitude','longitude', 'lead_time'], axis = 'columns') # valid_time is not found in the case of using target_class\n",
    "        yy_inpVar = yy_inp[['tp']]\n",
    "        y_train = yy_inpVar.values.ravel()\n",
    "\n",
    "        # testing data\n",
    "        x = hecm_int.mean(dim = 'realization').sel(forecast_time = slice('2018-01-01', '2019-12-30'))\n",
    "        xx = x.to_dataframe().reset_index()\n",
    "\n",
    "        # add the correlated inputs\n",
    "        xx['tp_lag1'] = xx.tp.shift(1).bfill()\n",
    "\n",
    "        nam = 0\n",
    "        for ii in dlo:\n",
    "            for jj in dla:\n",
    "\n",
    "                xx['Var'] = hecm.tp.sel(latitude = disla + jj, longitude = dislo + ii).sel(forecast_time = slice('2018-01-01', '2019-12-30')).mean(dim = 'realization')\n",
    "                name = 'var {}'.format(nam)\n",
    "                xx.rename(columns={'Var': name}, inplace=True)\n",
    "                nam = nam + 1\n",
    "\n",
    "        xx_dropna = xx.dropna(how='all', axis=1)\n",
    "        xx_inpVar = xx_dropna[x_inpVar.columns]\n",
    "\n",
    "        x_test = xx_inpVar.values\n",
    "\n",
    "        y = target_cl.sel(longitude = long, latitude = lat).sel(forecast_time = slice('2018-01-01', '2019-12-30'))\n",
    "        yy = y.to_dataframe().reset_index()\n",
    "        yy_inp = yy.drop(['latitude','longitude', 'lead_time'], axis = 'columns') # valid_time is not found in the case of using target_class\n",
    "        yy_inpVar = yy_inp[['tp']]\n",
    "        y_test = yy_inpVar.values.ravel()\n",
    "\n",
    "\n",
    "        # Building the Knn model\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors= 3)\n",
    "        \n",
    "        knn_model = knn.fit(x_train, y_train)\n",
    "\n",
    "        training_accuracy = knn_model.score(x_train, y_train)\n",
    "        \n",
    "        # Knn predictions\n",
    "\n",
    "        yy_pred = knn.predict_proba(x_test)\n",
    "        accuracy = knn_model.score(x_test, y_test)\n",
    "        \n",
    "        # add the values to the empty array\n",
    "        \n",
    "        prediction_an[:, j, i] = yy_pred[:,2]\n",
    "        prediction_nn[:, j, i] = yy_pred[:,1]\n",
    "        prediction_bn[:, j, i] = yy_pred[:,0]\n",
    "        \n",
    "        testing[:, j, i] = y_test\n",
    "        \n",
    "        training_acc[0, j, i] = training_accuracy \n",
    "        testing_acc[0, j, i] = accuracy   \n",
    "        \n",
    "        j = j + 1 \n",
    "    \n",
    "        print(1)\n",
    "        \n",
    "    i = i + 1\n",
    "\n",
    "# Make an xarray of predictions data\n",
    "\n",
    "time_step = [i for i in range(0, len(yy_pred))]\n",
    "\n",
    "longitude = longitude.values\n",
    "latitude = latitude.values\n",
    "\n",
    "predict_an = xr.DataArray(\n",
    "    data=prediction_an,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "        longitude=([\"longitude\"], longitude),\n",
    "        latitude=([\"latitude\"], latitude),      \n",
    "        time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "        description=\"predictions_an\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "predict_nn = xr.DataArray(\n",
    "    data=prediction_nn,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "        longitude=([\"longitude\"], longitude),\n",
    "        latitude=([\"latitude\"], latitude),      \n",
    "        time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "        description=\"predictions_nn\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "predict_bn = xr.DataArray(\n",
    "    data=prediction_bn,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "        longitude=([\"longitude\"], longitude),\n",
    "        latitude=([\"latitude\"], latitude),      \n",
    "        time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "        description=\"predictions_bn\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Make an xarray of tests data\n",
    "\n",
    "test = xr.DataArray(\n",
    "    data=testing,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "        longitude=([\"longitude\"], longitude),\n",
    "        latitude=([\"latitude\"], latitude),       \n",
    "        time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "        description=\"predictions\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# make categories from the predictions\n",
    "\n",
    "categories = xr.concat([predict_bn, predict_nn, predict_an],'category').assign_coords(category=['below normal', 'near normal', 'above normal'])\n",
    "\n",
    "categorical_p = categories.rename({'time_step':'forecast_time'})\n",
    "\n",
    "# Make an xarray for the testing accuracies\n",
    "\n",
    "test_acc = xr.DataArray(\n",
    "    data=testing_acc,\n",
    "    dims=[\"training_accuracy\", \"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "        longitude=([\"longitude\"], longitude),\n",
    "        latitude=([\"latitude\"], latitude),       \n",
    "    ),\n",
    "    attrs=dict(\n",
    "        description=\"testing accuracy\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Make an xarray for the training accuracies\n",
    "\n",
    "train_acc = xr.DataArray(\n",
    "    data=training_acc,\n",
    "    dims=[\"training_accuracy\", \"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "        longitude=([\"longitude\"], longitude),\n",
    "        latitude=([\"latitude\"], latitude),       \n",
    "    ),\n",
    "    attrs=dict(\n",
    "        description=\"testing accuracy\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xskillscore as xs\n",
    "from scripts import  make_probabilistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindcast_ecmwfter = make_probabilistic(hindcast_ecmwf, tercile_edges = tercile_edges)\n",
    "hindcast_observprob = make_probabilistic(hindcast_observation, tercile_edges = tercile_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del hindcast_ecmwf['week']\n",
    "del hindcast_ecmwf['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_p = hindcast_observprob['tp'].isel(lead_time = 0).sel(forecast_time = slice('2018-01-01', '2019-12-30'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_p = xr.DataArray([1/3, 1/3, 1/3], dims='category', coords={'category':['below normal', 'near normal', 'above normal']}).to_dataset(name='tp')\n",
    "clim_p['t2m'] = clim_p['tp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_p['forecast_time'] = obs_p['forecast_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region1 = rpss_cl.sel(longitude = slice(95, 100)).sel(latitude = slice(36, 31))\n",
    "# region2 = rpss_cl.sel(longitude = slice(100, 105)).sel(latitude = slice(60, 55))\n",
    "# region3 = rpss_cl.sel(longitude = slice(5, 10)).sel(latitude = slice(53, 48))\n",
    "\n",
    "def RPSS(region, categorical, start, end):\n",
    "        \n",
    "    # regions\n",
    "\n",
    "    region1 = hindcast_observation.sel(longitude = slice(95, 100)).sel(latitude = slice(36, 31))\n",
    "    region2 = hindcast_observation.sel(longitude = slice(100, 105)).sel(latitude = slice(60, 55))\n",
    "    region3 = hindcast_observation.sel(longitude = slice(5, 10)).sel(latitude = slice(53, 48))\n",
    "\n",
    "    if region == 'region1':\n",
    "        long = region1.longitude\n",
    "        lat  = region1.latitude\n",
    "        \n",
    "    if region == 'region2':\n",
    "        long = region2.longitude\n",
    "        lat  = region2.latitude\n",
    "    \n",
    "    if region == 'region3':\n",
    "        long = region3.longitude\n",
    "        lat  = region3.latitude\n",
    "    \n",
    "    obs_p = hindcast_observprob['tp'].isel(lead_time = 0).sel(forecast_time = slice(start, end)).sel(longitude = long).sel(latitude = lat)\n",
    "    ecmwf_p = hindcast_ecmwfter['tp'].isel(lead_time = 0).sel(forecast_time = slice(start, end)).sel(longitude = long).sel(latitude = lat)\n",
    "    \n",
    "    rps_cl = xs.rps(obs_p, clim_p['tp'], category_edges=None, dim=[], input_distributions='p').compute()\n",
    "    rps_ecmwf = xs.rps(obs_p, ecmwf_p, category_edges=None, dim=[], input_distributions='p').compute()\n",
    "    rps_model = xs.rps(obs_p, categorical, category_edges=None, dim=[], input_distributions='p').compute()\n",
    "    \n",
    "    rpss_cl = 1 - (rps_model.mean('forecast_time') / rps_cl.mean('forecast_time'))\n",
    "    rpss_ecmwf = 1 - (rps_model.mean('forecast_time') / rps_ecmwf.mean('forecast_time'))  \n",
    "    \n",
    "    RPSS_cl = rpss_cl.mean(dim = 'latitude').mean(dim = 'longitude').values\n",
    "    RPSS_ecmwf = rpss_ecmwf.mean(dim = 'latitude').mean(dim = 'longitude').values\n",
    "    \n",
    "    print(RPSS_cl, RPSS_ecmwf)\n",
    "    \n",
    "    return RPSS_cl, RPSS_ecmwf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RPSS_cl, RPSS_ecmwf = RPSS('region3', categorical_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_ML(dataset, probabilistic, tercile_edges, longitude, latitude, variable, lead_time, start, end):\n",
    "    aprob = probabilistic.sel(forecast_time= slice(start, end)).sel(longitude = longitude, latitude = latitude, method = 'nearest').isel(category = 2).values\n",
    "    nnprob = probabilistic.sel(forecast_time= slice(start, end)).sel(longitude = longitude, latitude = latitude, method = 'nearest').isel(category = 1).values\n",
    "    bprob = probabilistic.sel(forecast_time= slice(start, end)).sel(longitude = longitude, latitude = latitude, method = 'nearest').isel(category = 0).values\n",
    "    \n",
    "    x_plus = dataset[variable].sel(forecast_time= slice(start, end)).isel(lead_time = lead_time).sel(longitude = longitude, latitude = latitude, method = 'nearest')\n",
    "\n",
    "    upper = []\n",
    "    lower = []\n",
    "    for i in range(0,len(dataset.forecast_time)):\n",
    "        if i < 53:\n",
    "            u = tercile_edges[variable].isel(week = i, category_edge = 1, lead_time = lead_time).sel(longitude = longitude, latitude = latitude, method = 'nearest').values\n",
    "            upper.append(u)\n",
    "            l = tercile_edges[variable].isel(week = i, category_edge = 0, lead_time = lead_time).sel(longitude = longitude, latitude = latitude, method = 'nearest').values\n",
    "            lower.append(l)\n",
    "\n",
    "    for i in range(0,(int(len(dataset.forecast_time)/53)-1)):\n",
    "        for j in range(0, 53):\n",
    "            upper.append(upper[j])\n",
    "            lower.append(lower[j])\n",
    "            \n",
    "    dataset['upper']=(['forecast_time'], upper)\n",
    "    dataset['lower']=(['forecast_time'], lower)\n",
    "\n",
    "    u = dataset.upper.sel(forecast_time= slice(start, end))\n",
    "\n",
    "    l = dataset.lower.sel(forecast_time= slice(start, end))\n",
    "\n",
    "    \n",
    "    forecast_time = x_plus.forecast_time\n",
    "    \n",
    "    above = []\n",
    "    nnormal = []\n",
    "    below = []\n",
    "    \n",
    "    \n",
    "    for i in range(0, len(forecast_time)):\n",
    "        \n",
    "        if x_plus.isel(forecast_time = i).values > u.isel(forecast_time = i):\n",
    "            a = 1\n",
    "            above.append(a)\n",
    "        else:\n",
    "            a = 0\n",
    "            above.append(a)\n",
    "            \n",
    "        if l.isel(forecast_time = i) < x_plus.isel(forecast_time =  i) < u.isel(forecast_time = i):\n",
    "            nn = 1\n",
    "            nnormal.append(nn)\n",
    "        else:\n",
    "            nn = 0\n",
    "            nnormal.append(nn)\n",
    "        \n",
    "        if x_plus.isel(forecast_time = i) < l.isel(forecast_time = i):\n",
    "\n",
    "            b = 1\n",
    "            below.append(b)\n",
    "        else:\n",
    "            b = 0\n",
    "            below.append(b)\n",
    "    \n",
    "    \n",
    "    ns_probs = [0 for _ in range(len(aprob))]\n",
    "    # calculate scores\n",
    "    ns_auc = roc_auc_score(above, ns_probs)\n",
    "    nma_auc = roc_auc_score(above, aprob)\n",
    "    nmnn_auc = roc_auc_score(nnormal, nnprob)\n",
    "    nmb_auc = roc_auc_score(below, bprob)\n",
    "    \n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(above, ns_probs)\n",
    "    nma_fpr, nma_tpr, _ = roc_curve(above, aprob)\n",
    "    nmnn_fpr, nmnn_tpr, _ = roc_curve(nnormal, nnprob)\n",
    "    nmb_fpr, nmb_tpr, _ = roc_curve(below, bprob)\n",
    "    \n",
    "    # plot the ROC\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n",
    "    axes.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "    axes.plot(nma_fpr, nma_tpr, color = 'r', label = 'Above normal, AUC = {:.3f}'.format(nma_auc))\n",
    "    axes.plot(nmnn_fpr, nmnn_tpr, color = 'g', label = 'Near normal, AUC = {:.3f}'.format(nmnn_auc))\n",
    "    axes.plot(nmb_fpr, nmb_tpr, color = 'y', label = 'Below normal, AUC = {:.3f}'.format(nmb_auc))\n",
    "    axes.set_ylim(ymin = 0)\n",
    "    axes.set_xlim(xmin = 0)\n",
    "    axes.set(xlabel= 'False Positive Rate', ylabel='True Positive Rate', title = 'Receiver operating characteristic curve (ROC)')\n",
    "    axes.legend(loc=\"upper left\")\n",
    "    \n",
    "    del dataset['upper']\n",
    "    del dataset['lower']\n",
    "    \n",
    "    return aprob, nnprob, bprob, above, nnormal, below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region1 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 100, 36, 'tp', 0, '2018-01-01', '2019-12-30')\n",
    "# region2 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 100, 55, 'tp', 0, '2018-01-01', '2019-12-30')\n",
    "# NNl_roc3 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 6, 53, 'tp', 0, '2018-01-01', '2019-12-30')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression (LR), Multilayer perceptron (MLP) and random forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the random forest model is added to the logistic regression and the multi-layer perceptron model\n",
    "\n",
    "def ML_models(model, region):\n",
    "\n",
    "    # import the data\n",
    "\n",
    "    # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "    hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).chunk(dict(forecast_time=-1))\n",
    "    hobs = hindcast_observation.sel(forecast_time = slice('2000-01-01', '2012-12-30')).isel(lead_time  = 0).chunk(dict(forecast_time=-1))\n",
    "    hobs_int = hobs.interpolate_na(dim = 'forecast_time')\n",
    "    hecm_int = hecm.interpolate_na(dim = 'forecast_time')\n",
    "\n",
    "    target_cl = target_class.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).fillna(10).chunk(dict(forecast_time=-1))\n",
    "\n",
    "    # hecm_int['cl'] = target_cl \n",
    "    # hobs_int['cl'] = target_cl\n",
    "\n",
    "    # claculate the classes\n",
    "    obs00_19_terTR = obs00_19_ter.assign_coords(category=[0, 1, 2])\n",
    "    classes = (obs00_19_terTR * obs00_19_terTR.category).sum('category')\n",
    "    classes = classes.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).chunk(dict(forecast_time=-1))\n",
    "\n",
    "    \n",
    "    # modify the longitude and latitude to the selected regions:\n",
    "\n",
    "    # regions\n",
    "\n",
    "    region1 = hobs.sel(longitude = slice(95, 100)).sel(latitude = slice(36, 31))\n",
    "    region2 = hobs.sel(longitude = slice(100, 105)).sel(latitude = slice(60, 55))\n",
    "    region3 = hobs.sel(longitude = slice(5, 10)).sel(latitude = slice(53, 48))\n",
    "\n",
    "    if region == 'region1':\n",
    "        longitude = region1.longitude\n",
    "        latitude  = region1.latitude\n",
    "        \n",
    "    if region == 'region2':\n",
    "        longitude = region2.longitude\n",
    "        latitude  = region2.latitude\n",
    "    \n",
    "    if region == 'region3':\n",
    "        longitude = region3.longitude\n",
    "        latitude  = region3.latitude\n",
    "\n",
    "    # create the empty arrays for the results\n",
    "\n",
    "    x = hecm_int.mean(dim = 'realization').sel(forecast_time = slice('2018-01-01', '2019-12-30'))\n",
    "\n",
    "    prediction_an = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "    prediction_nn = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "    prediction_bn = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "    \n",
    "    testing = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "    \n",
    "    training_acc = np.zeros((1, len(latitude), len(longitude)))\n",
    "    testing_acc = np.zeros((1, len(latitude), len(longitude)))\n",
    "\n",
    "    i = 0\n",
    "    for long in longitude:\n",
    "\n",
    "        j = 0\n",
    "        for lat in latitude:\n",
    "\n",
    "            # choose the input and output data\n",
    "\n",
    "            # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "            hecm_cell = hecm.sel(longitude = long, latitude = lat)\n",
    "            hobs_cell = hobs.sel(longitude = long, latitude = lat)\n",
    "            hobs_int = hobs_cell.interpolate_na(dim = 'forecast_time')\n",
    "            hecm_int = hecm_cell.interpolate_na(dim = 'forecast_time')\n",
    "            # hobs_int['cl'] = target_cl\n",
    "\n",
    "            # operations\n",
    "            # x inputs\n",
    "\n",
    "            # training data\n",
    "\n",
    "            x = hecm_int.mean(dim = 'realization').sel(forecast_time = slice('2000-01-01', '2017-12-30'))\n",
    "            xx = x.to_dataframe().reset_index()\n",
    "            xx_inp = xx.drop(['latitude','longitude', 'lead_time', 'valid_time'], axis = 'columns')\n",
    "            xx_inpVar = xx_inp[['tp','t2m']]\n",
    "            x_train = xx_inpVar.values\n",
    "\n",
    "            y = target_cl.sel(longitude = long, latitude = lat).sel(forecast_time = slice('2000-01-01', '2017-12-30'))\n",
    "            yy = y.to_dataframe().reset_index()\n",
    "            yy_inp = yy.drop(['latitude','longitude', 'lead_time'], axis = 'columns') # valid_time is not found in the case of using target_class\n",
    "            yy_inpVar = yy_inp[['tp']]\n",
    "            y_train = yy_inpVar.values.ravel()\n",
    "\n",
    "            # testing data\n",
    "\n",
    "            x = hecm_int.mean(dim = 'realization').sel(forecast_time = slice('2018-01-01', '2019-12-30'))\n",
    "            xx = x.to_dataframe().reset_index()\n",
    "            xx_inp = xx.drop(['latitude','longitude', 'lead_time', 'valid_time'], axis = 'columns')\n",
    "            xx_inpVar = xx_inp[['tp','t2m']]\n",
    "            x_test = xx_inpVar.values\n",
    "\n",
    "            y = target_cl.sel(longitude = long, latitude = lat).sel(forecast_time = slice('2018-01-01', '2019-12-30'))\n",
    "            yy = y.to_dataframe().reset_index()\n",
    "            yy_inp = yy.drop(['latitude','longitude', 'lead_time'], axis = 'columns') # valid_time is not found in the case of using target_class\n",
    "            yy_inpVar = yy_inp[['tp']]\n",
    "            y_test = yy_inpVar.values.ravel()\n",
    "\n",
    "\n",
    "            # Training the LR and MLP models\n",
    "            \n",
    "            min_max_scaler = preprocessing.MinMaxScaler()\n",
    "            x_trainT = min_max_scaler.fit_transform(x_train)\n",
    "            x_testT = min_max_scaler.fit_transform(x_test)\n",
    "\n",
    "            if  model == 'LR':\n",
    "                lr = LogisticRegression()\n",
    "                lr_model = lr.fit(x_trainT, y_train)\n",
    "                training_accuracy = lr_model.score(x_train, y_train)\n",
    "#                 print(training_accuracy)\n",
    "\n",
    "            if model == 'RF':\n",
    "                \n",
    "                rf = RandomForestClassifier()\n",
    "                rf_model = rf.fit(x_trainT, y_train)\n",
    "                training_accuracy = rf_model.score(x_train, y_train)\n",
    "\n",
    "            if model == 'MLP':\n",
    "        \n",
    "                # define model\n",
    "                mlp_model = Sequential()\n",
    "                mlp_model.add(Dense(500, input_dim=2, activation='relu'))\n",
    "                mlp_model.add(Dense(3, activation='softmax'))\n",
    "                mlp_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "                # simple early stopping\n",
    "                es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "                mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "                # fit model\n",
    "                history = mlp_model.fit(x_trainT, y_train, validation_data=(x_testT, y_test), epochs=4000, verbose=0, callbacks=[es, mc])\n",
    "\n",
    "                # load the saved model\n",
    "                saved_model = load_model('best_model.h5')\n",
    "\n",
    "                # evaluate the model\n",
    "                _, train_acc = saved_model.evaluate(x_trainT, y_train, verbose=0)\n",
    "                _, test_acc = saved_model.evaluate(x_testT, y_test, verbose=0)\n",
    "                training_accuracy = train_acc\n",
    "                accuracy = test_acc\n",
    "\n",
    "            # Testing the LR and MLP models\n",
    "            \n",
    "            if  model == 'LR':\n",
    "                yy_pred = lr_model.predict_proba(x_testT)\n",
    "                accuracy = lr_model.score(x_testT, y_test)\n",
    "                \n",
    "            if model == 'RF':\n",
    "                yy_pred = rf_model.predict_proba(x_testT)\n",
    "                accuracy = rf_model.score(x_testT, y_test) \n",
    "                \n",
    "            if model == 'MLP':\n",
    "                yy_pred = saved_model.predict(x_testT)\n",
    "\n",
    "                \n",
    "            # add the values to the empty array\n",
    "            \n",
    "            prediction_an[:, j, i] = yy_pred[:,2]\n",
    "            prediction_nn[:, j, i] = yy_pred[:,1]\n",
    "            prediction_bn[:, j, i] = yy_pred[:,0]\n",
    "\n",
    "            testing[:, j, i] = y_test\n",
    "            training_acc[0, j, i] = training_accuracy \n",
    "            testing_acc[0, j, i] = accuracy          \n",
    "        \n",
    "            j = j + 1 \n",
    "\n",
    "            print(1)\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    # Make an xarray of predictions data\n",
    "    \n",
    "    time_step = [i for i in range(0, len(yy_pred))]\n",
    "    \n",
    "    longitude = longitude.values\n",
    "    latitude = latitude.values\n",
    "\n",
    "\n",
    "    predict_an = xr.DataArray(\n",
    "        data=prediction_an,\n",
    "        dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),      \n",
    "            time_step= ([\"time_step\"], time_step)\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"predictions_an\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    predict_nn = xr.DataArray(\n",
    "        data=prediction_nn,\n",
    "        dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),      \n",
    "            time_step= ([\"time_step\"], time_step)\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"predictions_nn\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    predict_bn = xr.DataArray(\n",
    "        data=prediction_bn,\n",
    "        dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),      \n",
    "            time_step= ([\"time_step\"], time_step)\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"predictions_bn\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # make categories from the predictions\n",
    "\n",
    "    categories = xr.concat([predict_bn, predict_nn, predict_an],'category').assign_coords(category=['below normal', 'near normal', 'above normal'])\n",
    "    categorical_p = categories.rename({'time_step':'forecast_time'})\n",
    "    \n",
    "\n",
    "    # Make an xarray of tests data\n",
    "\n",
    "    test = xr.DataArray(\n",
    "        data=testing,\n",
    "        dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "            time_step= ([\"time_step\"], time_step)\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"predictions\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Make an xarray for the testing accuracies\n",
    "    \n",
    "    test_acc = xr.DataArray(\n",
    "        data=testing_acc,\n",
    "        dims=[\"testing_accuracy\", \"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"testing accuracy\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Make an xarray for the training accuracies\n",
    "    \n",
    "    train_acc = xr.DataArray(\n",
    "        data=training_acc,\n",
    "        dims=[\"training_accuracy\", \"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"testing accuracy\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return test, test_acc, train_acc, categorical_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, test_acc, train_acc, categorical_p = ML_models('RF', 'region3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_p['forecast_time'] = obs_p['forecast_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RPSS_cl, RPSS_ecmwf = RPSS('region3', categorical_p, '2018-01-01', '2019-12-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region1 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 100, 36, 'tp', 0, '2018-01-01', '2019-12-30')\n",
    "# region2 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 100, 55, 'tp', 0, '2018-01-01', '2019-12-30')\n",
    "NNl_roc3 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 6, 53, 'tp', 0, '2018-01-01', '2019-12-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training and testing data are added using one loop and the iloc command is used to select the training and testing datasets\n",
    "# the random forest model is added to the logistic regression and the multi-layer perceptron model\n",
    "\n",
    "def ML_models_cor(model, region, spatial):\n",
    "\n",
    "    # import the data\n",
    "\n",
    "    # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "    hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).chunk(dict(forecast_time=-1))\n",
    "    hobs = hindcast_observation.sel(forecast_time = slice('2000-01-01', '2012-12-30')).isel(lead_time  = 0).chunk(dict(forecast_time=-1))\n",
    "    hobs_int = hobs.interpolate_na(dim = 'forecast_time')\n",
    "    hecm_int = hecm.interpolate_na(dim = 'forecast_time')\n",
    "\n",
    "    target_cl = target_class.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).fillna(10).chunk(dict(forecast_time=-1))\n",
    "\n",
    "    # hecm_int['cl'] = target_cl \n",
    "    # hobs_int['cl'] = target_cl\n",
    "\n",
    "    # claculate the classes\n",
    "    obs00_19_terTR = obs00_19_ter.assign_coords(category=[0, 1, 2])\n",
    "    classes = (obs00_19_terTR * obs00_19_terTR.category).sum('category')\n",
    "    classes = classes.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).chunk(dict(forecast_time=-1))\n",
    "\n",
    "    \n",
    "    # modify the longitude and latitude to the selected regions:\n",
    "\n",
    "    # regions\n",
    "\n",
    "    region1 = hobs.sel(longitude = slice(95, 100)).sel(latitude = slice(36, 31))\n",
    "    region2 = hobs.sel(longitude = slice(100, 105)).sel(latitude = slice(60, 55))\n",
    "    region3 = hobs.sel(longitude = slice(5, 10)).sel(latitude = slice(53, 48))\n",
    "\n",
    "    if region == 'region1':\n",
    "        longitude = region1.longitude\n",
    "        latitude  = region1.latitude\n",
    "        \n",
    "    if region == 'region2':\n",
    "        longitude = region2.longitude\n",
    "        latitude  = region2.latitude\n",
    "    \n",
    "    if region == 'region3':\n",
    "        longitude = region3.longitude\n",
    "        latitude  = region3.latitude\n",
    "\n",
    "    # create the empty arrays for the results\n",
    "\n",
    "    x = hecm_int.mean(dim = 'realization').sel(forecast_time = slice('2018-01-02', '2019-12-24'))\n",
    "\n",
    "    prediction_an = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "    prediction_nn = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "    prediction_bn = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "    \n",
    "    testing = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "    \n",
    "    training_acc = np.zeros((1, len(latitude), len(longitude)))\n",
    "    testing_acc = np.zeros((1, len(latitude), len(longitude)))\n",
    "\n",
    "    i = 0\n",
    "    for long in longitude:\n",
    "\n",
    "        j = 0\n",
    "        for lat in latitude:\n",
    "\n",
    "            # choose the input and output data\n",
    "\n",
    "            # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "            hecm_cell = hecm.sel(longitude = long, latitude = lat)\n",
    "            hobs_cell = hobs.sel(longitude = long, latitude = lat)\n",
    "            hobs_int = hobs_cell.interpolate_na(dim = 'forecast_time')\n",
    "            hecm_int = hecm_cell.interpolate_na(dim = 'forecast_time')\n",
    "            # hobs_int['cl'] = target_cl\n",
    "\n",
    "            # operations\n",
    "            # x inputs\n",
    "\n",
    "            # training data\n",
    "\n",
    "            x = hecm_int.mean(dim = 'realization').sel(forecast_time = slice('2000-01-01', '2019-12-30'))\n",
    "            xx = x.to_dataframe().reset_index()\n",
    "            \n",
    "            # add the correlated inputs\n",
    "            xx['tp_lag1'] = xx.tp.shift(1).bfill()\n",
    "            \n",
    "            if spatial == 'one':\n",
    "                disla = lat - (1 * 1.5)\n",
    "                dislo = long - (1 * 1.5)\n",
    "                dla = np.arange(0, 4.5, 1.5)\n",
    "                dlo = np.arange(0, 4.5, 1.5)\n",
    "                v = 'var 4'\n",
    "                \n",
    "            if spatial == 'two':\n",
    "                disla = lat - (2 * 1.5)\n",
    "                dislo = long - (2 * 1.5)\n",
    "                dla = np.arange(0, 7.5, 1.5)\n",
    "                dlo = np.arange(0, 7.5, 1.5)\n",
    "                v = 'var 12'\n",
    "                \n",
    "            if spatial == 'three':\n",
    "                disla = lat - (3 * 1.5)\n",
    "                dislo = long - (3 * 1.5)\n",
    "                dla = np.arange(0, 10.5, 1.5)\n",
    "                dlo = np.arange(0, 10.5, 1.5)\n",
    "                v = 'var 24'\n",
    "                \n",
    "            # var = []\n",
    "            nam = 0\n",
    "            for ii in dlo:\n",
    "                for jj in dla:\n",
    "\n",
    "                    xx['Var'] = hecm.tp.sel(latitude = disla + jj, longitude = dislo + ii).sel(forecast_time = slice('2000-01-01', '2019-12-30')).mean(dim = 'realization')\n",
    "                    name = 'var {}'.format(nam)\n",
    "                    xx.rename(columns={'Var': name}, inplace=True)\n",
    "                    nam = nam + 1\n",
    "                    \n",
    "            \n",
    "            xx_dropna = xx.dropna(how='all', axis=1)\n",
    "            xx_inp_train = xx_dropna.loc[0:953]\n",
    "            xx_inp_tr = xx_inp_train.drop(['forecast_time','latitude','longitude', 'lead_time', 'valid_time'], axis = 'columns')\n",
    "            xx_inpVar_train = xx_inp_tr.drop([v], axis = 'columns')\n",
    "            \n",
    "            x_train = xx_inpVar_train.values\n",
    "\n",
    "            y = target_cl.sel(longitude = long, latitude = lat).sel(forecast_time = slice('2000-01-01', '2019-12-30'))\n",
    "            yy = y.to_dataframe().reset_index()\n",
    "            yy_inp = yy.drop(['latitude','longitude', 'lead_time'], axis = 'columns') # valid_time is not found in the case of using target_class\n",
    "            yy_inpVar = yy_inp[['tp']]\n",
    "            \n",
    "            y_train = yy_inpVar.loc[0:953].values.ravel()\n",
    "\n",
    "            # testing data\n",
    "            xx_inp_test = xx_dropna.loc[954:]\n",
    "            xx_inp_te = xx_inp_test.drop(['forecast_time','latitude','longitude', 'lead_time', 'valid_time'], axis = 'columns')\n",
    "            xx_inpVar_test = xx_inp_te.drop([v], axis = 'columns')\n",
    "            \n",
    "            x_test = xx_inpVar_test.values\n",
    "\n",
    "            y_test = yy_inpVar.loc[954:].values.ravel()\n",
    "\n",
    "            # Training the LR and MLP models\n",
    "            \n",
    "            min_max_scaler = preprocessing.MinMaxScaler()\n",
    "            x_trainT = min_max_scaler.fit_transform(x_train)\n",
    "            x_testT = min_max_scaler.fit_transform(x_test)\n",
    "\n",
    "            if  model == 'LR':\n",
    "                lr = LogisticRegression()\n",
    "                lr_model = lr.fit(x_trainT, y_train)\n",
    "                training_accuracy = lr_model.score(x_train, y_train)\n",
    "#                 print(training_accuracy)\n",
    "\n",
    "            if model == 'RF':\n",
    "                \n",
    "                rf = RandomForestClassifier()\n",
    "                rf_model = rf.fit(x_trainT, y_train)\n",
    "                training_accuracy = rf_model.score(x_train, y_train)\n",
    "\n",
    "            if model == 'MLP':\n",
    "        \n",
    "                # define model\n",
    "                mlp_model = Sequential()\n",
    "                mlp_model.add(Dense(500, input_dim=len(xx_inpVar_train.columns), activation='relu'))\n",
    "                mlp_model.add(Dense(3, activation='softmax'))\n",
    "                mlp_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "                # simple early stopping\n",
    "                es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "                mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "                # fit model\n",
    "                history = mlp_model.fit(x_trainT, y_train, validation_data=(x_testT, y_test), epochs=4000, verbose=0, callbacks=[es, mc])\n",
    "\n",
    "                # load the saved model\n",
    "                saved_model = load_model('best_model.h5')\n",
    "\n",
    "                # evaluate the model\n",
    "                _, train_acc = saved_model.evaluate(x_trainT, y_train, verbose=0)\n",
    "                _, test_acc = saved_model.evaluate(x_testT, y_test, verbose=0)\n",
    "                training_accuracy = train_acc\n",
    "                accuracy = test_acc\n",
    "\n",
    "            # Testing the LR and MLP models\n",
    "            \n",
    "            if  model == 'LR':\n",
    "                yy_pred = lr_model.predict_proba(x_testT)\n",
    "                accuracy = lr_model.score(x_testT, y_test)\n",
    "                \n",
    "            if model == 'RF':\n",
    "                yy_pred = rf_model.predict_proba(x_testT)\n",
    "                accuracy = rf_model.score(x_testT, y_test)\n",
    "            \n",
    "            if model == 'MLP':\n",
    "                yy_pred = saved_model.predict(x_testT)\n",
    "\n",
    "                \n",
    "            # add the values to the empty array\n",
    "            \n",
    "            prediction_an[:, j, i] = yy_pred[:,2]\n",
    "            prediction_nn[:, j, i] = yy_pred[:,1]\n",
    "            prediction_bn[:, j, i] = yy_pred[:,0]\n",
    "\n",
    "            testing[:, j, i] = y_test\n",
    "            training_acc[0, j, i] = training_accuracy \n",
    "            testing_acc[0, j, i] = accuracy          \n",
    "        \n",
    "            j = j + 1 \n",
    "\n",
    "            print(1)\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    # Make an xarray of predictions data\n",
    "    \n",
    "    time_step = [i for i in range(0, len(yy_pred))]\n",
    "    \n",
    "    longitude = longitude.values\n",
    "    latitude = latitude.values\n",
    "\n",
    "\n",
    "    predict_an = xr.DataArray(\n",
    "        data=prediction_an,\n",
    "        dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),      \n",
    "            time_step= ([\"time_step\"], time_step)\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"predictions_an\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    predict_nn = xr.DataArray(\n",
    "        data=prediction_nn,\n",
    "        dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),      \n",
    "            time_step= ([\"time_step\"], time_step)\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"predictions_nn\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    predict_bn = xr.DataArray(\n",
    "        data=prediction_bn,\n",
    "        dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),      \n",
    "            time_step= ([\"time_step\"], time_step)\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"predictions_bn\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # make categories from the predictions\n",
    "\n",
    "    categories = xr.concat([predict_bn, predict_nn, predict_an],'category').assign_coords(category=['below normal', 'near normal', 'above normal'])\n",
    "    categorical_p = categories.rename({'time_step':'forecast_time'})\n",
    "    \n",
    "\n",
    "    # Make an xarray of tests data\n",
    "\n",
    "    test = xr.DataArray(\n",
    "        data=testing,\n",
    "        dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "            time_step= ([\"time_step\"], time_step)\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"predictions\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Make an xarray for the testing accuracies\n",
    "    \n",
    "    test_acc = xr.DataArray(\n",
    "        data=testing_acc,\n",
    "        dims=[\"testing_accuracy\", \"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"testing accuracy\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Make an xarray for the training accuracies\n",
    "    \n",
    "    train_acc = xr.DataArray(\n",
    "        data=training_acc,\n",
    "        dims=[\"training_accuracy\", \"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"testing accuracy\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return xx_inp_train, test, test_acc, train_acc, categorical_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_inp_train, test, test_acc, train_acc, categorical_p = ML_models_cor('RF', 'region3', 'two')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_p['forecast_time'] = obs_p['forecast_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RPSS_cl, RPSS_ecmwf = RPSS('region2', categorical_p, '2018-01-01', '2019-12-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region1 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 100, 36, 'tp', 0, '2018-01-01', '2019-12-30')\n",
    "region2 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 100, 55, 'tp', 0, '2018-01-01', '2019-12-30')\n",
    "# NNl_roc3 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 6, 53, 'tp', 0, '2018-01-01', '2019-12-30')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from numpy import hstack\n",
    "from numpy import insert\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ML_LSTM(region):\n",
    "    lead_time = 0\n",
    "    # import the data\n",
    "\n",
    "    # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "    hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).chunk(dict(forecast_time=-1))\n",
    "    hobs = hindcast_observation.sel(forecast_time = slice('2000-01-01', '2012-12-30')).isel(lead_time  = 0).chunk(dict(forecast_time=-1))\n",
    "    hobs_int = hobs.interpolate_na(dim = 'forecast_time')\n",
    "    hecm_int = hecm.interpolate_na(dim = 'forecast_time')\n",
    "\n",
    "    target_cl = target_class.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).fillna(10).chunk(dict(forecast_time=-1))\n",
    "\n",
    "    # hecm_int['cl'] = target_cl \n",
    "    # hobs_int['cl'] = target_cl\n",
    "\n",
    "    # claculate the classes\n",
    "    obs00_19_terTR = obs00_19_ter.assign_coords(category=[0, 1, 2])\n",
    "    classes = (obs00_19_terTR * obs00_19_terTR.category).sum('category')\n",
    "    classes = classes.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).chunk(dict(forecast_time=-1))\n",
    "\n",
    "    \n",
    "    # modify the longitude and latitude to the selected regions:\n",
    "\n",
    "    # regions\n",
    "\n",
    "    region1 = hobs.sel(longitude = slice(95, 100)).sel(latitude = slice(36, 31))\n",
    "    region2 = hobs.sel(longitude = slice(100, 105)).sel(latitude = slice(60, 55))\n",
    "    region3 = hobs.sel(longitude = slice(5, 10)).sel(latitude = slice(53, 48))\n",
    "\n",
    "    if region == 'region1':\n",
    "        longitude = region1.longitude\n",
    "        latitude  = region1.latitude\n",
    "        \n",
    "    if region == 'region2':\n",
    "        longitude = region2.longitude\n",
    "        latitude  = region2.latitude\n",
    "    \n",
    "    if region == 'region3':\n",
    "        longitude = region3.longitude\n",
    "        latitude  = region3.latitude\n",
    "\n",
    "    # create the empty arrays for the results\n",
    "\n",
    "    x = hecm_int.mean(dim = 'realization').sel(forecast_time = slice('2018-01-01', '2019-12-30'))\n",
    "\n",
    "    prediction_an = np.zeros((105, len(latitude), len(longitude)))\n",
    "    prediction_nn = np.zeros((105, len(latitude), len(longitude)))\n",
    "    prediction_bn = np.zeros((105, len(latitude), len(longitude)))\n",
    "    \n",
    "    training_acc = np.zeros((1, len(latitude), len(longitude)))\n",
    "    testing_acc = np.zeros((1, len(latitude), len(longitude)))\n",
    "\n",
    "    i = 0\n",
    "    for long in longitude:\n",
    "\n",
    "        j = 0\n",
    "        for lat in latitude:\n",
    "\n",
    "            # choose the input and output data\n",
    "\n",
    "            # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "            hecm_cell = hecm.sel(longitude = long, latitude = lat)\n",
    "            hobs_cell = hobs.sel(longitude = long, latitude = lat)\n",
    "            hobs_int = hobs_cell.interpolate_na(dim = 'forecast_time')\n",
    "            hecm_int = hecm_cell.interpolate_na(dim = 'forecast_time')\n",
    "            # hobs_int['cl'] = target_cl\n",
    "\n",
    "            # training and testing data\n",
    "\n",
    "            x = hecm_int.mean(dim = 'realization').sel(forecast_time = slice('2000-01-01', '2019-12-30'))\n",
    "            xx = x.to_dataframe().reset_index()\n",
    "            xx_inp = xx.drop(['latitude','longitude', 'lead_time', 'valid_time'], axis = 'columns')\n",
    "            xx_inpVar = xx_inp[['tp','t2m']]\n",
    "\n",
    "            y = target_cl.sel(longitude = long, latitude = lat, method = 'nearest').sel(forecast_time = slice('2000-01-01', '2019-12-30'))\n",
    "            yy = y.to_dataframe().reset_index()\n",
    "            yy_inp = yy.drop(['latitude','longitude', 'lead_time'], axis = 'columns') # valid_time is not found in the case of using target_class\n",
    "            yy_inpVar = yy_inp[['tp']]\n",
    "\n",
    "            x_train = xx_inpVar.loc[0:953]\n",
    "            x_train_values = x_train.values\n",
    "\n",
    "            x_test = xx_inpVar.loc[901:]\n",
    "            x_test_values = x_test.values\n",
    "\n",
    "\n",
    "            y_train = yy_inpVar.loc[0:953]\n",
    "            y_train_values = y_train.values.ravel()\n",
    "\n",
    "            y_test = yy_inpVar.loc[901:]\n",
    "            y_test_values = y_test.values.ravel()\n",
    "\n",
    "            # using the minimum and maximum scalar to normalize the data\n",
    "\n",
    "            x_trainT = min_max_scaler.fit_transform(x_train_values)\n",
    "            x_testT = min_max_scaler.fit_transform(x_test_values)\n",
    "\n",
    "            # data preprocessing for the LSTM technique\n",
    "            # for training\n",
    "\n",
    "            ytrdata = insert(y_train_values, 0, 1000)\n",
    "            print('the target ouptut shape for training')\n",
    "            print(ytrdata.shape)\n",
    "\n",
    "            xtrtp = insert(x_trainT[:,0], (954), 1000)\n",
    "            xtrt2m = insert(x_trainT[:,1], (954), 1000)\n",
    "\n",
    "            xtrtpre = xtrtp.reshape(len(xtrtp), 1)\n",
    "            xtrt2mre = xtrt2m.reshape(len(xtrt2m), 1)\n",
    "\n",
    "            xtrdata = hstack((xtrtpre, xtrt2mre))\n",
    "            print('training input shape')\n",
    "            print(xtrdata.shape)\n",
    "\n",
    "            # for testing\n",
    "\n",
    "            ytedata = insert(y_test_values, 0, 1000)\n",
    "            print('the target ouptut shape for testing')\n",
    "            print(ytedata.shape)\n",
    "\n",
    "            xtetp = insert(x_testT[:,0], (158), 1000)\n",
    "            xtet2m = insert(x_testT[:,1], (158), 1000)\n",
    "\n",
    "            xtetpre = xtetp.reshape(len(xtetp), 1)\n",
    "            xtet2mre = xtet2m.reshape(len(xtet2m), 1)\n",
    "\n",
    "            xtedata = hstack((xtetpre, xtet2mre))\n",
    "            print('testing input shape')\n",
    "            print(xtedata.shape)\n",
    "\n",
    "            # using the timeseries generator to generate the data\n",
    "            n_input = 54\n",
    "            trgenerator = TimeseriesGenerator(xtrdata, ytrdata, length=n_input, batch_size=1)\n",
    "            tegenerator = TimeseriesGenerator(xtedata, ytedata, length=n_input, batch_size=1)\n",
    "\n",
    "            # built the LSTM architecture\n",
    "\n",
    "            # define model\n",
    "\n",
    "            lstm_model = Sequential()\n",
    "            lstm_model.add(LSTM(100, input_shape= (n_input, x_train_values.shape[1]), activation='relu'))\n",
    "            lstm_model.add(Dense(3, activation='softmax'))\n",
    "            lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "            # simple early stopping\n",
    "            es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "            mc = ModelCheckpoint('models/LSTM'+ region +'/model_' + str(lead_time) + '_' +  str(i) + str(j) + '.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "            # fit the model\n",
    "            lstm_model.fit(trgenerator, validation_data= tegenerator, epochs=4000, verbose=0, callbacks=[es, mc])\n",
    "\n",
    "            # load the model\n",
    "            Model = load_model('models/LSTM' + region + '/model_' + str(lead_time) + '_' +  str(i) + str(j) + '.h5')\n",
    "\n",
    "            # evaluate the model\n",
    "            _, train_acc = Model.evaluate(trgenerator, verbose=0)\n",
    "            _, test_acc = Model.evaluate(tegenerator, verbose=0)\n",
    "            training_accuracy = train_acc\n",
    "            accuracy = test_acc\n",
    "\n",
    "            # make predictions\n",
    "            yy_pred = np.zeros((len(tegenerator),3))\n",
    "\n",
    "            for g in range(len(tegenerator)):\n",
    "                x, y = tegenerator[g]\n",
    "                ypredict = Model.predict(x)\n",
    "                yy_pred[g, :] = ypredict\n",
    "            \n",
    "            # add the values to the empty array   \n",
    "            prediction_an[:, j, i] = yy_pred[:,2]\n",
    "            prediction_nn[:, j, i] = yy_pred[:,1]\n",
    "            prediction_bn[:, j, i] = yy_pred[:,0]\n",
    "\n",
    "            training_acc[0, j, i] = training_accuracy \n",
    "            testing_acc[0, j, i] = accuracy          \n",
    "        \n",
    "            j = j + 1 \n",
    "\n",
    "            print(1)\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    # Make an xarray of predictions data\n",
    "    \n",
    "    time_step = [i for i in range(0, len(yy_pred))]\n",
    "    \n",
    "    longitude = longitude.values\n",
    "    latitude = latitude.values\n",
    "\n",
    "\n",
    "    predict_an = xr.DataArray(\n",
    "        data=prediction_an,\n",
    "        dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),      \n",
    "            time_step= ([\"time_step\"], time_step)\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"predictions_an\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    predict_nn = xr.DataArray(\n",
    "        data=prediction_nn,\n",
    "        dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),      \n",
    "            time_step= ([\"time_step\"], time_step)\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"predictions_nn\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    predict_bn = xr.DataArray(\n",
    "        data=prediction_bn,\n",
    "        dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),      \n",
    "            time_step= ([\"time_step\"], time_step)\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"predictions_bn\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # make categories from the predictions\n",
    "\n",
    "    categories = xr.concat([predict_bn, predict_nn, predict_an],'category').assign_coords(category=['below normal', 'near normal', 'above normal'])\n",
    "    categorical_p = categories.rename({'time_step':'forecast_time'})\n",
    "    \n",
    "    # Make an xarray for the testing accuracies\n",
    "    \n",
    "    test_acc = xr.DataArray(\n",
    "        data=testing_acc,\n",
    "        dims=[\"testing_accuracy\", \"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"testing accuracy\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Make an xarray for the training accuracies\n",
    "    \n",
    "    train_acc = xr.DataArray(\n",
    "        data=training_acc,\n",
    "        dims=[\"training_accuracy\", \"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"testing accuracy\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return test_acc, train_acc, categorical_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training and testing data are added using one loop and the iloc command is used to select the training and testing datasets\n",
    "# the random forest model is added to the logistic regression and the multi-layer perceptron model\n",
    "\n",
    "def ML_LSTM_cor(region, spatial):\n",
    "\n",
    "    # import the data\n",
    "\n",
    "    # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "    hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).chunk(dict(forecast_time=-1))\n",
    "    hobs = hindcast_observation.sel(forecast_time = slice('2000-01-01', '2012-12-30')).isel(lead_time  = 0).chunk(dict(forecast_time=-1))\n",
    "    hobs_int = hobs.interpolate_na(dim = 'forecast_time')\n",
    "    hecm_int = hecm.interpolate_na(dim = 'forecast_time')\n",
    "\n",
    "    target_cl = target_class.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).fillna(10).chunk(dict(forecast_time=-1))\n",
    "\n",
    "    # hecm_int['cl'] = target_cl \n",
    "    # hobs_int['cl'] = target_cl\n",
    "\n",
    "    # claculate the classes\n",
    "    obs00_19_terTR = obs00_19_ter.assign_coords(category=[0, 1, 2])\n",
    "    classes = (obs00_19_terTR * obs00_19_terTR.category).sum('category')\n",
    "    classes = classes.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).chunk(dict(forecast_time=-1))\n",
    "\n",
    "    \n",
    "    # modify the longitude and latitude to the selected regions:\n",
    "\n",
    "    # regions\n",
    "\n",
    "    region1 = hobs.sel(longitude = slice(95, 100)).sel(latitude = slice(36, 31))\n",
    "    region2 = hobs.sel(longitude = slice(100, 105)).sel(latitude = slice(60, 55))\n",
    "    region3 = hobs.sel(longitude = slice(5, 10)).sel(latitude = slice(53, 48))\n",
    "\n",
    "    if region == 'region1':\n",
    "        longitude = region1.longitude\n",
    "        latitude  = region1.latitude\n",
    "        \n",
    "    if region == 'region2':\n",
    "        longitude = region2.longitude\n",
    "        latitude  = region2.latitude\n",
    "    \n",
    "    if region == 'region3':\n",
    "        longitude = region3.longitude\n",
    "        latitude  = region3.latitude\n",
    "\n",
    "    # create the empty arrays for the results\n",
    "\n",
    "    x = hecm_int.mean(dim = 'realization').sel(forecast_time = slice('2018-01-02', '2019-12-24'))\n",
    "\n",
    "    prediction_an = np.zeros((105, len(latitude), len(longitude)))\n",
    "    prediction_nn = np.zeros((105, len(latitude), len(longitude)))\n",
    "    prediction_bn = np.zeros((105, len(latitude), len(longitude)))\n",
    "    \n",
    "    training_acc = np.zeros((1, len(latitude), len(longitude)))\n",
    "    testing_acc = np.zeros((1, len(latitude), len(longitude)))\n",
    "\n",
    "    i = 0\n",
    "    for long in longitude:\n",
    "\n",
    "        j = 0\n",
    "        for lat in latitude:\n",
    "\n",
    "            # choose the input and output data\n",
    "\n",
    "            # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "            hecm_cell = hecm.sel(longitude = long, latitude = lat)\n",
    "            hobs_cell = hobs.sel(longitude = long, latitude = lat)\n",
    "            hobs_int = hobs_cell.interpolate_na(dim = 'forecast_time')\n",
    "            hecm_int = hecm_cell.interpolate_na(dim = 'forecast_time')\n",
    "            # hobs_int['cl'] = target_cl\n",
    "\n",
    "            # operations\n",
    "            # x inputs\n",
    "\n",
    "            # training data\n",
    "\n",
    "            x = hecm_int.mean(dim = 'realization').sel(forecast_time = slice('2000-01-01', '2019-12-30'))\n",
    "            xx = x.to_dataframe().reset_index()\n",
    "            \n",
    "            # add the correlated inputs\n",
    "            xx['tp_lag1'] = xx.tp.shift(1).bfill()\n",
    "            \n",
    "            if spatial == 'one':\n",
    "                disla = lat - (1 * 1.5)\n",
    "                dislo = long - (1 * 1.5)\n",
    "                dla = np.arange(0, 4.5, 1.5)\n",
    "                dlo = np.arange(0, 4.5, 1.5)\n",
    "                v = 'var 4'\n",
    "                \n",
    "            if spatial == 'two':\n",
    "                disla = lat - (2 * 1.5)\n",
    "                dislo = long - (2 * 1.5)\n",
    "                dla = np.arange(0, 7.5, 1.5)\n",
    "                dlo = np.arange(0, 7.5, 1.5)\n",
    "                v = 'var 12'\n",
    "                \n",
    "            if spatial == 'three':\n",
    "                disla = lat - (3 * 1.5)\n",
    "                dislo = long - (3 * 1.5)\n",
    "                dla = np.arange(0, 10.5, 1.5)\n",
    "                dlo = np.arange(0, 10.5, 1.5)\n",
    "                v = 'var 24'\n",
    "                \n",
    "            # var = []\n",
    "            nam = 0\n",
    "            for ii in dlo:\n",
    "                for jj in dla:\n",
    "\n",
    "                    xx['Var'] = hecm.tp.sel(latitude = disla + jj, longitude = dislo + ii).sel(forecast_time = slice('2000-01-01', '2019-12-30')).mean(dim = 'realization')\n",
    "                    name = 'var {}'.format(nam)\n",
    "                    xx.rename(columns={'Var': name}, inplace=True)\n",
    "                    nam = nam + 1\n",
    "            \n",
    "            xx_dropna = xx.dropna(how='all', axis=1)\n",
    "            xx_inp_train = xx_dropna.loc[0:953]\n",
    "            \n",
    "            # adding one row to the end of the data\n",
    "            new_data = pd.DataFrame(xx_inp_train[-1:].values,  index=[954], columns= xx_inp_train.columns)\n",
    "            xx_inp_train = xx_inp_train.append(new_data)\n",
    "            \n",
    "            xx_inp_tr = xx_inp_train.drop(['forecast_time','latitude','longitude', 'lead_time', 'valid_time'], axis = 'columns')\n",
    "            xx_inpVar_train = xx_inp_tr.drop([v], axis = 'columns')\n",
    "            \n",
    "            x_train = xx_inpVar_train.values\n",
    "\n",
    "            y = target_cl.sel(longitude = long, latitude = lat).sel(forecast_time = slice('2000-01-01', '2019-12-30'))\n",
    "            yy = y.to_dataframe().reset_index()\n",
    "            yy_inp = yy.drop(['latitude','longitude', 'lead_time'], axis = 'columns') # valid_time is not found in the case of using target_class\n",
    "            yy_inpVar = yy_inp[['tp']]\n",
    "            \n",
    "            y_train = yy_inpVar.loc[0:953].values.ravel()\n",
    "\n",
    "            # testing data\n",
    "            xx_inp_test = xx_dropna.loc[901:]\n",
    "            \n",
    "            # adding one row to the end of the data\n",
    "            new_data = pd.DataFrame(xx_inp_test[-1:].values,  index=[159], columns= xx_inp_test.columns)\n",
    "            xx_inp_test = xx_inp_test.append(new_data)\n",
    "            \n",
    "            xx_inp_te = xx_inp_test.drop(['forecast_time','latitude','longitude', 'lead_time', 'valid_time'], axis = 'columns')\n",
    "            xx_inpVar_test = xx_inp_te.drop([v], axis = 'columns')\n",
    "            \n",
    "            x_test = xx_inpVar_test.values\n",
    "\n",
    "            y_test = yy_inpVar.loc[901:].values.ravel()\n",
    "\n",
    "            # Training the LR and MLP models\n",
    "            \n",
    "            min_max_scaler = preprocessing.MinMaxScaler()\n",
    "            x_trainT = min_max_scaler.fit_transform(x_train)\n",
    "            x_testT = min_max_scaler.fit_transform(x_test)\n",
    "\n",
    "            # data preprocessing for the LSTM technique\n",
    "            # for training\n",
    "\n",
    "            ytrdata = insert(y_train, 0, 1000)\n",
    "            print('the target ouptut shape for training')\n",
    "            print(ytrdata.shape)\n",
    "            \n",
    "            xtrdata = x_trainT\n",
    "            print('training input shape')\n",
    "            print(xtrdata.shape)\n",
    "\n",
    "            # for testing\n",
    "\n",
    "            ytedata = insert(y_test, 0, 1000)\n",
    "            print('the target ouptut shape for testing')\n",
    "            print(ytedata.shape)\n",
    "\n",
    "            xtedata = x_testT\n",
    "            print('testing input shape')\n",
    "            print(xtedata.shape)\n",
    "\n",
    "            # using the timeseries generator to generate the data\n",
    "            n_input = 54\n",
    "            trgenerator = TimeseriesGenerator(xtrdata, ytrdata, length=n_input, batch_size=1)\n",
    "            tegenerator = TimeseriesGenerator(xtedata, ytedata, length=n_input, batch_size=1)\n",
    "\n",
    "            # built the LSTM architecture\n",
    "\n",
    "            # define model\n",
    "\n",
    "            lstm_model = Sequential()\n",
    "            lstm_model.add(LSTM(100, input_shape= (n_input, x_train_values.shape[1]), activation='relu'))\n",
    "            lstm_model.add(Dense(3, activation='softmax'))\n",
    "            lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "            # simple early stopping\n",
    "            es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "            mc = ModelCheckpoint('models/LSTM'+ region +'/model_' + str(lead_time) + str(r + 1) + '_' +  str(i) + str(j) + '.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)''\n",
    "\n",
    "            # fit the model\n",
    "            lstm_model.fit(trgenerator, validation_data= tegenerator, epochs=4000, verbose=0, callbacks=[es, mc])\n",
    "\n",
    "            # load the model\n",
    "            Model = load_model('models/LSTM' + region + '/model_' + str(lead_time) + str(r + 1) + '_' +  str(i) + str(j) + '.h5')\n",
    "            \n",
    "            # evaluate the model\n",
    "            _, train_acc = Model.evaluate(trgenerator, verbose=0)\n",
    "            _, test_acc = Model.evaluate(tegenerator, verbose=0)\n",
    "            training_accuracy = train_acc\n",
    "            accuracy = test_acc\n",
    "\n",
    "            # make predictions\n",
    "            yy_pred = np.zeros((len(tegenerator),3))\n",
    "\n",
    "            for g in range(len(tegenerator)):\n",
    "                x, y = tegenerator[g]\n",
    "                ypredict = Model.predict(x)\n",
    "                yy_pred[g, :] = ypredict\n",
    "            \n",
    "            # add the values to the empty array\n",
    "            \n",
    "            prediction_an[:, j, i] = yy_pred[:,2]\n",
    "            prediction_nn[:, j, i] = yy_pred[:,1]\n",
    "            prediction_bn[:, j, i] = yy_pred[:,0]\n",
    "\n",
    "            training_acc[0, j, i] = training_accuracy \n",
    "            testing_acc[0, j, i] = accuracy          \n",
    "        \n",
    "            j = j + 1 \n",
    "\n",
    "            print(1)\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    # Make an xarray of predictions data\n",
    "    \n",
    "    time_step = [i for i in range(0, len(yy_pred))]\n",
    "    \n",
    "    longitude = longitude.values\n",
    "    latitude = latitude.values\n",
    "\n",
    "\n",
    "    predict_an = xr.DataArray(\n",
    "        data=prediction_an,\n",
    "        dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),      \n",
    "            time_step= ([\"time_step\"], time_step)\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"predictions_an\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    predict_nn = xr.DataArray(\n",
    "        data=prediction_nn,\n",
    "        dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),      \n",
    "            time_step= ([\"time_step\"], time_step)\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"predictions_nn\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    predict_bn = xr.DataArray(\n",
    "        data=prediction_bn,\n",
    "        dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),      \n",
    "            time_step= ([\"time_step\"], time_step)\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"predictions_bn\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # make categories from the predictions\n",
    "\n",
    "    categories = xr.concat([predict_bn, predict_nn, predict_an],'category').assign_coords(category=['below normal', 'near normal', 'above normal'])\n",
    "    categorical_p = categories.rename({'time_step':'forecast_time'})\n",
    "    \n",
    "    # Make an xarray for the testing accuracies\n",
    "    \n",
    "    test_acc = xr.DataArray(\n",
    "        data=testing_acc,\n",
    "        dims=[\"testing_accuracy\", \"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"testing accuracy\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Make an xarray for the training accuracies\n",
    "    \n",
    "    train_acc = xr.DataArray(\n",
    "        data=training_acc,\n",
    "        dims=[\"training_accuracy\", \"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"testing accuracy\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return xx_inp_train, test_acc, train_acc, categorical_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_inp_train, test_acc, train_acc, categorical_p = ML_LSTM_cor('RF', 'region3', 'two')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_p['forecast_time'] = obs_p['forecast_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RPSS_cl, RPSS_ecmwf = RPSS('region2', categorical_p, '2018-01-01', '2019-12-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region1 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 100, 36, 'tp', 0, '2018-01-01', '2019-12-30')\n",
    "region2 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 100, 55, 'tp', 0, '2018-01-01', '2019-12-30')\n",
    "# NNl_roc3 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 6, 53, 'tp', 0, '2018-01-01', '2019-12-30')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Committee model (CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function of the stacked model\n",
    "def CM(region, model, lead_time):\n",
    "\n",
    "\n",
    "    # import the data\n",
    "\n",
    "    # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "    hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).chunk(dict(forecast_time=-1))\n",
    "    hobs = hindcast_observation.sel(forecast_time = slice('2000-01-01', '2012-12-30')).isel(lead_time  = 0).chunk(dict(forecast_time=-1))\n",
    "    hobs_int = hobs.interpolate_na(dim = 'forecast_time')\n",
    "    hecm_int = hecm.interpolate_na(dim = 'forecast_time')\n",
    "\n",
    "    target_cl = target_class.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).fillna(10).chunk(dict(forecast_time=-1))\n",
    "\n",
    "    # hecm_int['cl'] = target_cl \n",
    "    # hobs_int['cl'] = target_cl\n",
    "\n",
    "    # modify the longitude and latitude to the selected regions:\n",
    "\n",
    "    # regions\n",
    "\n",
    "    region1 = hobs.sel(longitude = slice(95, 100)).sel(latitude = slice(36, 31))\n",
    "    region2 = hobs.sel(longitude = slice(100, 105)).sel(latitude = slice(60, 55))\n",
    "    region3 = hobs.sel(longitude = slice(5, 10)).sel(latitude = slice(53, 48))\n",
    "\n",
    "    if region == 'region1':\n",
    "        longitude = region1.longitude\n",
    "        latitude  = region1.latitude\n",
    "\n",
    "    if region == 'region2':\n",
    "        longitude = region2.longitude\n",
    "        latitude  = region2.latitude\n",
    "\n",
    "    if region == 'region3':\n",
    "        longitude = region3.longitude\n",
    "        latitude  = region3.latitude\n",
    "\n",
    "    # create the empty arrays for the results\n",
    "    x = hecm_int.mean(dim = 'realization').sel(forecast_time = slice('2018-01-01', '2019-12-30'))\n",
    "\n",
    "    prediction_an = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "    prediction_nn = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "    prediction_bn = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "\n",
    "    testing = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "\n",
    "    training_acc = np.zeros((1, len(latitude), len(longitude)))\n",
    "    testing_acc = np.zeros((1, len(latitude), len(longitude)))\n",
    "\n",
    "    i = 0\n",
    "    for long in longitude:\n",
    "\n",
    "        j = 0\n",
    "        for lat in latitude:\n",
    "\n",
    "            stackX = None\n",
    "\n",
    "            # choose the input and output data\n",
    "\n",
    "            # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "            hecm_cell = hecm.sel(longitude = long, latitude = lat)\n",
    "            hobs_cell = hobs.sel(longitude = long, latitude = lat)\n",
    "            hobs_int = hobs_cell.interpolate_na(dim = 'forecast_time')\n",
    "            hecm_int = hecm_cell.interpolate_na(dim = 'forecast_time')\n",
    "            # hobs_int['cl'] = target_cl\n",
    "\n",
    "            # operations\n",
    "            # x inputs\n",
    "\n",
    "            y = target_cl.sel(longitude = long, latitude = lat).sel(forecast_time = slice('2000-01-01', '2019-12-30'))\n",
    "            yy = y.to_dataframe().reset_index()\n",
    "            yy_inp = yy.drop(['latitude','longitude', 'lead_time'], axis = 'columns') # valid_time is not found in the case of using target_class\n",
    "            yy_inpVar = yy_inp[['tp']]\n",
    "\n",
    "            yy_inpVar_train = yy_inpVar.loc[0:953]\n",
    "            y_train = yy_inpVar_train.values.ravel()\n",
    "\n",
    "            yy_inpVar_test = yy_inpVar.loc[954:]\n",
    "            y_test = yy_inpVar_test.values.ravel()\n",
    "\n",
    "            for r in range(0,11): \n",
    "\n",
    "                # training and testing data\n",
    "\n",
    "                x = hecm_int.isel(realization = r).sel(forecast_time = slice('2000-01-01', '2019-12-30'))\n",
    "                xx = x.to_dataframe().reset_index()\n",
    "                xx_inp = xx.drop(['latitude','longitude', 'lead_time', 'valid_time'], axis = 'columns')\n",
    "                xx_inpVar = xx_inp[['tp','t2m']]\n",
    "\n",
    "                xx_inpVar_train = xx_inpVar.loc[0:953]\n",
    "                x_train = xx_inpVar_train.values\n",
    "\n",
    "                xx_inpVar_test = xx_inpVar.loc[954:]\n",
    "                x_test = xx_inpVar_test.values\n",
    "\n",
    "                # Training the LR and MLP models\n",
    "\n",
    "                min_max_scaler = preprocessing.MinMaxScaler()\n",
    "                x_trainT = min_max_scaler.fit_transform(x_train)\n",
    "                x_testT = min_max_scaler.fit_transform(x_test)\n",
    "\n",
    "                if  model == 'LR':\n",
    "                    lr = LogisticRegression()\n",
    "                    lr_model = lr.fit(x_trainT, y_train)\n",
    "                    training_accuracy = lr_model.score(x_train, y_train)\n",
    "    #                 print(training_accuracy)\n",
    "\n",
    "                if model == 'MLP':\n",
    "\n",
    "                    # define model\n",
    "                    mlp_model = Sequential()\n",
    "                    mlp_model.add(Dense(500, input_dim=len(xx_inpVar_train.columns), activation='relu'))\n",
    "                    mlp_model.add(Dense(3, activation='softmax'))\n",
    "                    mlp_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "                    # simple early stopping\n",
    "                    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "                    mc = ModelCheckpoint('models/'+ region +'/model_' + str(lead_time) + str(r + 1) + '_' +  str(i) + str(j) + '.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "                    # fit model\n",
    "                    Model = mlp_model.fit(x_trainT, y_train, validation_data=(x_testT, y_test), epochs=4000, verbose=0, callbacks=[es, mc])        \n",
    "\n",
    "                    # load the model\n",
    "                    Model = load_model('models/' + region + '/model_' + str(lead_time) + str(r + 1) + '_' +  str(i) + str(j) + '.h5')\n",
    "\n",
    "                    # make prediction\n",
    "                    yhat = Model.predict(x_testT, verbose=0)\n",
    "\n",
    "                    # stack predictions into [rows, members, probabilities]\n",
    "                    if stackX is None:\n",
    "                        stackX = yhat\n",
    "                    else:\n",
    "                        stackX = dstack((stackX, yhat))\n",
    "\n",
    "            stackXX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "\n",
    "            meta_model = LogisticRegression()\n",
    "\n",
    "            meta_model.fit(stackXX, y_test)\n",
    "\n",
    "            training_accuracy = meta_model.score(stackXX, y_test)\n",
    "            accuracy = training_accuracy\n",
    "\n",
    "            yy_pred = meta_model.predict_proba(stackXX)\n",
    "\n",
    "            # save the meta_model\n",
    "            pickle.dump(meta_model, open('models/' + region + '/meta_model_' + str(lead_time) + '_' +  str(i) + str(j) + '.sav', 'wb'))\n",
    "\n",
    "            # add the values to the empty array\n",
    "\n",
    "            prediction_an[:, j, i] = yy_pred[:,2]\n",
    "            prediction_nn[:, j, i] = yy_pred[:,1]\n",
    "            prediction_bn[:, j, i] = yy_pred[:,0]\n",
    "\n",
    "            testing[:, j, i] = y_test\n",
    "            training_acc[0, j, i] = training_accuracy \n",
    "            testing_acc[0, j, i] = accuracy   \n",
    "\n",
    "            j = j + 1 \n",
    "\n",
    "            print(1)\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    # Make an xarray of predictions data\n",
    "\n",
    "    time_step = [i for i in range(0, len(yy_pred))]\n",
    "\n",
    "    longitude = longitude.values\n",
    "    latitude = latitude.values\n",
    "\n",
    "\n",
    "    predict_an = xr.DataArray(\n",
    "    data=prediction_an,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "    longitude=([\"longitude\"], longitude),\n",
    "    latitude=([\"latitude\"], latitude),      \n",
    "    time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "    description=\"predictions_an\",\n",
    "    ),\n",
    "    )\n",
    "\n",
    "    predict_nn = xr.DataArray(\n",
    "    data=prediction_nn,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "    longitude=([\"longitude\"], longitude),\n",
    "    latitude=([\"latitude\"], latitude),      \n",
    "    time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "    description=\"predictions_nn\",\n",
    "    ),\n",
    "    )\n",
    "\n",
    "    predict_bn = xr.DataArray(\n",
    "    data=prediction_bn,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "    longitude=([\"longitude\"], longitude),\n",
    "    latitude=([\"latitude\"], latitude),      \n",
    "    time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "    description=\"predictions_bn\",\n",
    "    ),\n",
    "    )\n",
    "\n",
    "    # make categories from the predictions\n",
    "\n",
    "    categories = xr.concat([predict_bn, predict_nn, predict_an],'category').assign_coords(category=['below normal', 'near normal', 'above normal'])\n",
    "    categorical_p = categories.rename({'time_step':'forecast_time'})\n",
    "\n",
    "    # Make an xarray of tests data\n",
    "\n",
    "    test = xr.DataArray(\n",
    "        data=testing,\n",
    "        dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "            time_step= ([\"time_step\"], time_step)\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"predictions\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Make an xarray for the testing accuracies\n",
    "\n",
    "    test_acc = xr.DataArray(\n",
    "        data=testing_acc,\n",
    "        dims=[\"testing_accuracy\", \"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"testing accuracy\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Make an xarray for the training accuracies\n",
    "\n",
    "    train_acc = xr.DataArray(\n",
    "        data=training_acc,\n",
    "        dims=[\"training_accuracy\", \"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"testing accuracy\",\n",
    "        ),\n",
    "    )\n",
    "    return test_acc, train_acc, categorical_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, train_acc, categorical_p = CM('region1', 'MLP', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_p = hindcast_observprob['tp'].isel(lead_time = 0).sel(forecast_time = slice('2018-01-01', '2019-12-30'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_p['forecast_time'] = obs_p['forecast_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region1 = rpss_cl.sel(longitude = slice(95, 100)).sel(latitude = slice(36, 31))\n",
    "# region2 = rpss_cl.sel(longitude = slice(100, 105)).sel(latitude = slice(60, 55))\n",
    "# region3 = rpss_cl.sel(longitude = slice(5, 10)).sel(latitude = slice(53, 48))\n",
    "\n",
    "def RPSS12(region, categorical, lead_time, start, end):\n",
    "        \n",
    "    # regions\n",
    "\n",
    "    region1 = hindcast_observation.sel(longitude = slice(95, 100)).sel(latitude = slice(36, 31))\n",
    "    region2 = hindcast_observation.sel(longitude = slice(100, 105)).sel(latitude = slice(60, 55))\n",
    "    region3 = hindcast_observation.sel(longitude = slice(5, 10)).sel(latitude = slice(53, 48))\n",
    "\n",
    "    if region == 'region1':\n",
    "        long = region1.longitude\n",
    "        lat  = region1.latitude\n",
    "        \n",
    "    if region == 'region2':\n",
    "        long = region2.longitude\n",
    "        lat  = region2.latitude\n",
    "    \n",
    "    if region == 'region3':\n",
    "        long = region3.longitude\n",
    "        lat  = region3.latitude\n",
    "    \n",
    "    obs_p = hindcast_observprob['tp'].isel(lead_time = lead_time).sel(forecast_time = slice(start, end)).sel(longitude = long).sel(latitude = lat)\n",
    "    ecmwf_p = hindcast_ecmwfter['tp'].isel(lead_time = lead_time).sel(forecast_time = slice(start, end)).sel(longitude = long).sel(latitude = lat)\n",
    "    \n",
    "    rps_cl = xs.rps(obs_p, clim_p['tp'], category_edges=None, dim=[], input_distributions='p').compute()\n",
    "    rps_ecmwf = xs.rps(obs_p, ecmwf_p, category_edges=None, dim=[], input_distributions='p').compute()\n",
    "    rps_model = xs.rps(obs_p, categorical, category_edges=None, dim=[], input_distributions='p').compute()\n",
    "    \n",
    "    rpss_cl = 1 - (rps_model.mean('forecast_time') / rps_cl.mean('forecast_time'))\n",
    "    rpss_ecmwf = 1 - (rps_model.mean('forecast_time') / rps_ecmwf.mean('forecast_time'))  \n",
    "    \n",
    "    RPSS_cl = rpss_cl.mean(dim = 'latitude').mean(dim = 'longitude').values\n",
    "    RPSS_ecmwf = rpss_ecmwf.mean(dim = 'latitude').mean(dim = 'longitude').values\n",
    "    \n",
    "    print(RPSS_cl, RPSS_ecmwf)\n",
    "    \n",
    "    return RPSS_cl, RPSS_ecmwf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RPSS_cl, RPSS_ecmwf = RPSS12('region3', categorical_p, 0, '2018-01-01', '2019-12-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region1 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 100, 36, 'tp', 0, '2018-01-01', '2019-12-30')\n",
    "# region2 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 100, 55, 'tp', 0, '2018-01-01', '2019-12-30')\n",
    "NNl_roc3 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 6, 53, 'tp', 0, '2018-01-01', '2019-12-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function of the stacked model\n",
    "# Adding the correlated inputs\n",
    "\n",
    "def CMcor(region, spatial, model, lead_time):\n",
    "\n",
    "    # import the data\n",
    "\n",
    "    # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "    hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).chunk(dict(forecast_time=-1))\n",
    "    hobs = hindcast_observation.sel(forecast_time = slice('2000-01-01', '2012-12-30')).isel(lead_time  = 0).chunk(dict(forecast_time=-1))\n",
    "    hobs_int = hobs.interpolate_na(dim = 'forecast_time')\n",
    "    hecm_interpolate = hecm.interpolate_na(dim = 'forecast_time')\n",
    "\n",
    "    target_cl = target_class.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).fillna(10).chunk(dict(forecast_time=-1))\n",
    "\n",
    "    # hecm_int['cl'] = target_cl \n",
    "    # hobs_int['cl'] = target_cl\n",
    "\n",
    "    # modify the longitude and latitude to the selected regions:\n",
    "\n",
    "    # regions\n",
    "\n",
    "    region1 = hobs.sel(longitude = slice(95, 100)).sel(latitude = slice(36, 31))\n",
    "    region2 = hobs.sel(longitude = slice(100, 105)).sel(latitude = slice(60, 55))\n",
    "    region3 = hobs.sel(longitude = slice(5, 10)).sel(latitude = slice(53, 48))\n",
    "\n",
    "    if region == 'region1':\n",
    "        longitude = region1.longitude\n",
    "        latitude  = region1.latitude\n",
    "\n",
    "    if region == 'region2':\n",
    "        longitude = region2.longitude\n",
    "        latitude  = region2.latitude\n",
    "\n",
    "    if region == 'region3':\n",
    "        longitude = region3.longitude\n",
    "        latitude  = region3.latitude\n",
    "\n",
    "    # create the empty arrays for the results\n",
    "    x = hecm_interpolate.mean(dim = 'realization').sel(forecast_time = slice('2018-01-01', '2019-12-30'))\n",
    "\n",
    "    prediction_an = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "    prediction_nn = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "    prediction_bn = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "\n",
    "    testing = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "\n",
    "    training_acc = np.zeros((1, len(latitude), len(longitude)))\n",
    "    testing_acc = np.zeros((1, len(latitude), len(longitude)))\n",
    "\n",
    "    i = 0\n",
    "    for long in longitude:\n",
    "\n",
    "        j = 0\n",
    "        for lat in latitude:\n",
    "\n",
    "            stackX = None\n",
    "\n",
    "            # choose the input and output data\n",
    "\n",
    "            # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "            hecm_cell = hecm.sel(longitude = long, latitude = lat)\n",
    "            hobs_cell = hobs.sel(longitude = long, latitude = lat)\n",
    "            hobs_int = hobs_cell.interpolate_na(dim = 'forecast_time')\n",
    "            hecm_int = hecm_cell.interpolate_na(dim = 'forecast_time')\n",
    "            # hobs_int['cl'] = target_cl\n",
    "\n",
    "            # operations\n",
    "            # adding the target outputs for training and testing\n",
    "\n",
    "            y = target_cl.sel(longitude = long, latitude = lat).sel(forecast_time = slice('2000-01-01', '2019-12-30'))\n",
    "            yy = y.to_dataframe().reset_index()\n",
    "            yy_inp = yy.drop(['latitude','longitude', 'lead_time'], axis = 'columns') # valid_time is not found in the case of using target_class\n",
    "            yy_inpVar = yy_inp[['tp']]\n",
    "\n",
    "            yy_inpVar_train = yy_inpVar.loc[0:953]\n",
    "            y_train = yy_inpVar_train.values.ravel()\n",
    "\n",
    "            yy_inpVar_test = yy_inpVar.loc[954:]\n",
    "            y_test = yy_inpVar_test.values.ravel()\n",
    "\n",
    "            for r in range(0,11): \n",
    "\n",
    "                # add the correlated inputs\n",
    "                x = hecm_int.isel(realization = r).sel(forecast_time = slice('2000-01-01', '2019-12-30'))\n",
    "                xx = x.to_dataframe().reset_index()\n",
    "                xx['tp_lag1'] = xx.tp.shift(1).bfill()\n",
    "\n",
    "                if spatial == 'one':\n",
    "                    disla = lat - (1 * 1.5)\n",
    "                    dislo = long - (1 * 1.5)\n",
    "                    dla = np.arange(0, 4.5, 1.5)\n",
    "                    dlo = np.arange(0, 4.5, 1.5)\n",
    "                    v = 'var 4'\n",
    "\n",
    "                if spatial == 'two':\n",
    "                    disla = lat - (2 * 1.5)\n",
    "                    dislo = long - (2 * 1.5)\n",
    "                    dla = np.arange(0, 7.5, 1.5)\n",
    "                    dlo = np.arange(0, 7.5, 1.5)\n",
    "                    v = 'var 12'\n",
    "\n",
    "                if spatial == 'three':\n",
    "                    disla = lat - (3 * 1.5)\n",
    "                    dislo = long - (3 * 1.5)\n",
    "                    dla = np.arange(0, 10.5, 1.5)\n",
    "                    dlo = np.arange(0, 10.5, 1.5)\n",
    "                    v = 'var 24'\n",
    "\n",
    "                # var = []\n",
    "                nam = 0\n",
    "                for ii in dlo:\n",
    "                    for jj in dla:\n",
    "\n",
    "                        xx['Var'] = hecm_interpolate.tp.sel(latitude = disla + jj, longitude = dislo + ii).sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(realization = r)\n",
    "                        name = 'var {}'.format(nam)\n",
    "                        xx.rename(columns={'Var': name}, inplace=True)\n",
    "                        nam = nam + 1\n",
    "\n",
    "\n",
    "                xx_dropna = xx.dropna(how='all', axis=1)\n",
    "                xx_inp_train = xx_dropna.loc[0:953]\n",
    "                xx_inp_tr = xx_inp_train.drop(['forecast_time','latitude','longitude', 'lead_time', 'valid_time', 'realization'], axis = 'columns')\n",
    "                xx_inpVar_train = xx_inp_tr.drop([v], axis = 'columns')\n",
    "\n",
    "                x_train = xx_inpVar_train.values\n",
    "                print(x_train.shape)\n",
    "\n",
    "                # testing data\n",
    "                xx_inp_test = xx_dropna.loc[954:]\n",
    "                xx_inp_te = xx_inp_test.drop(['forecast_time','latitude','longitude', 'lead_time', 'valid_time', 'realization'], axis = 'columns')\n",
    "                xx_inpVar_test = xx_inp_te.drop([v], axis = 'columns')\n",
    "\n",
    "                x_test = xx_inpVar_test.values\n",
    "\n",
    "                # Training the LR and MLP models\n",
    "\n",
    "                min_max_scaler = preprocessing.MinMaxScaler()\n",
    "                x_trainT = min_max_scaler.fit_transform(x_train)\n",
    "                x_testT = min_max_scaler.fit_transform(x_test)\n",
    "\n",
    "                if  model == 'LR':\n",
    "                    lr = LogisticRegression()\n",
    "                    lr_model = lr.fit(x_trainT, y_train)\n",
    "                    training_accuracy = lr_model.score(x_train, y_train)\n",
    "    #                 print(training_accuracy)\n",
    "\n",
    "                if model == 'MLP':\n",
    "\n",
    "                    # define model\n",
    "                    mlp_model = Sequential()\n",
    "                    mlp_model.add(Dense(500, input_dim=len(xx_inpVar_train.columns), activation='relu'))\n",
    "                    mlp_model.add(Dense(3, activation='softmax'))\n",
    "                    mlp_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "                    # simple early stopping\n",
    "                    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "                    mc = ModelCheckpoint('models/'+ region + spatial + '/model_' + str(lead_time) + str(r + 1) + '_' +  str(i) + str(j) + '.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "                    # fit model\n",
    "                    Model = mlp_model.fit(x_trainT, y_train, validation_data=(x_testT, y_test), epochs=4000, verbose=0, callbacks=[es, mc])        \n",
    "\n",
    "                    # load the model\n",
    "                    Model = load_model('models/' + region + spatial + '/model_' + str(lead_time) + str(r + 1) + '_' +  str(i) + str(j) + '.h5')\n",
    "\n",
    "                    # make prediction\n",
    "                    yhat = Model.predict(x_testT, verbose=0)\n",
    "\n",
    "                    # stack predictions into [rows, members, probabilities]\n",
    "                    if stackX is None:\n",
    "                        stackX = yhat\n",
    "                    else:\n",
    "                        stackX = dstack((stackX, yhat))\n",
    "\n",
    "            stackXX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "\n",
    "            stackXX = np.nan_to_num(stackXX)\n",
    "\n",
    "            meta_model = LogisticRegression()\n",
    "\n",
    "            meta_model.fit(stackXX, y_test)\n",
    "\n",
    "            training_accuracy = meta_model.score(stackXX, y_test)\n",
    "            accuracy = training_accuracy\n",
    "\n",
    "            yy_pred = meta_model.predict_proba(stackXX)\n",
    "\n",
    "            # save the meta_model\n",
    "            pickle.dump(meta_model, open('models/' + region + spatial + '/meta_model_' + str(lead_time) + '_' +  str(i) + str(j) + '.sav', 'wb'))\n",
    "\n",
    "            # add the values to the empty array\n",
    "\n",
    "            prediction_an[:, j, i] = yy_pred[:,2]\n",
    "            prediction_nn[:, j, i] = yy_pred[:,1]\n",
    "            prediction_bn[:, j, i] = yy_pred[:,0]\n",
    "\n",
    "            testing[:, j, i] = y_test\n",
    "            training_acc[0, j, i] = training_accuracy \n",
    "            testing_acc[0, j, i] = accuracy   \n",
    "\n",
    "            j = j + 1 \n",
    "\n",
    "            print(1)\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    # Make an xarray of predictions data\n",
    "\n",
    "    time_step = [i for i in range(0, len(yy_pred))]\n",
    "\n",
    "    longitude = longitude.values\n",
    "    latitude = latitude.values\n",
    "\n",
    "\n",
    "    predict_an = xr.DataArray(\n",
    "    data=prediction_an,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "    longitude=([\"longitude\"], longitude),\n",
    "    latitude=([\"latitude\"], latitude),      \n",
    "    time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "    description=\"predictions_an\",\n",
    "    ),\n",
    "    )\n",
    "\n",
    "    predict_nn = xr.DataArray(\n",
    "    data=prediction_nn,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "    longitude=([\"longitude\"], longitude),\n",
    "    latitude=([\"latitude\"], latitude),      \n",
    "    time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "    description=\"predictions_nn\",\n",
    "    ),\n",
    "    )\n",
    "\n",
    "    predict_bn = xr.DataArray(\n",
    "    data=prediction_bn,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "    longitude=([\"longitude\"], longitude),\n",
    "    latitude=([\"latitude\"], latitude),      \n",
    "    time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "    description=\"predictions_bn\",\n",
    "    ),\n",
    "    )\n",
    "\n",
    "    # make categories from the predictions\n",
    "\n",
    "    categories = xr.concat([predict_bn, predict_nn, predict_an],'category').assign_coords(category=['below normal', 'near normal', 'above normal'])\n",
    "    categorical_p = categories.rename({'time_step':'forecast_time'})\n",
    "\n",
    "    # Make an xarray of tests data\n",
    "\n",
    "    test = xr.DataArray(\n",
    "        data=testing,\n",
    "        dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "            time_step= ([\"time_step\"], time_step)\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"predictions\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Make an xarray for the testing accuracies\n",
    "\n",
    "    test_acc = xr.DataArray(\n",
    "        data=testing_acc,\n",
    "        dims=[\"testing_accuracy\", \"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"testing accuracy\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Make an xarray for the training accuracies\n",
    "\n",
    "    train_acc = xr.DataArray(\n",
    "        data=training_acc,\n",
    "        dims=[\"training_accuracy\", \"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"testing accuracy\",\n",
    "        ),\n",
    "    )\n",
    "    return test_acc, train_acc, categorical_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, train_acc, categorical_p = CMcor('region1', 'one' , 'MLP', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_p = hindcast_observprob['tp'].isel(lead_time = 0).sel(forecast_time = slice('2018-01-01', '2019-12-30'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_p['forecast_time'] = obs_p['forecast_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RPSS_cl, RPSS_ecmwf = RPSS12('region3', categorical_p, 0, '2018-01-01', '2019-12-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region1 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 100, 36, 'tp', 0, '2018-01-01', '2019-12-30')\n",
    "# region2 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 100, 55, 'tp', 0, '2018-01-01', '2019-12-30')\n",
    "NNl_roc3 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 6, 53, 'tp', 0, '2018-01-01', '2019-12-30')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this case, the same procedure from above is used, the only difference is that, two of the last three years are used for CV and last year is used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function of the stacked model\n",
    "# Using cross-validation and testing data\n",
    "\n",
    "def CM2cv1te(region, model):\n",
    "\n",
    "    # import the data\n",
    "\n",
    "    # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "    hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).chunk(dict(forecast_time=-1))\n",
    "    hobs = hindcast_observation.sel(forecast_time = slice('2000-01-01', '2012-12-30')).isel(lead_time  = 0).chunk(dict(forecast_time=-1))\n",
    "    hobs_int = hobs.interpolate_na(dim = 'forecast_time')\n",
    "    hecm_int = hecm.interpolate_na(dim = 'forecast_time')\n",
    "\n",
    "    target_cl = target_class.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).fillna(10).chunk(dict(forecast_time=-1))\n",
    "\n",
    "    # hecm_int['cl'] = target_cl \n",
    "    # hobs_int['cl'] = target_cl\n",
    "\n",
    "    # modify the longitude and latitude to the selected regions:\n",
    "\n",
    "    # regions\n",
    "\n",
    "    region1 = hobs.sel(longitude = slice(95, 100)).sel(latitude = slice(36, 31))\n",
    "    region2 = hobs.sel(longitude = slice(100, 105)).sel(latitude = slice(60, 55))\n",
    "    region3 = hobs.sel(longitude = slice(5, 10)).sel(latitude = slice(53, 48))\n",
    "\n",
    "    if region == 'region1':\n",
    "        longitude = region1.longitude\n",
    "        latitude  = region1.latitude\n",
    "\n",
    "    if region == 'region2':\n",
    "        longitude = region2.longitude\n",
    "        latitude  = region2.latitude\n",
    "\n",
    "    if region == 'region3':\n",
    "        longitude = region3.longitude\n",
    "        latitude  = region3.latitude\n",
    "\n",
    "    # create the empty arrays for the results\n",
    "    x = hecm_int.mean(dim = 'realization').sel(forecast_time = slice('2019-01-01', '2019-12-30'))\n",
    "\n",
    "    prediction_an = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "    prediction_nn = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "    prediction_bn = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "\n",
    "    testing = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "\n",
    "    training_acc = np.zeros((1, len(latitude), len(longitude)))\n",
    "    testing_acc = np.zeros((1, len(latitude), len(longitude)))\n",
    "\n",
    "    i = 0\n",
    "    for long in longitude:\n",
    "\n",
    "        j = 0\n",
    "        for lat in latitude:\n",
    "\n",
    "            stackX = None\n",
    "\n",
    "            # choose the input and output data\n",
    "\n",
    "            # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "            hecm_cell = hecm.sel(longitude = long, latitude = lat)\n",
    "            hobs_cell = hobs.sel(longitude = long, latitude = lat)\n",
    "            hobs_int = hobs_cell.interpolate_na(dim = 'forecast_time')\n",
    "            hecm_int = hecm_cell.interpolate_na(dim = 'forecast_time')\n",
    "            # hobs_int['cl'] = target_cl\n",
    "\n",
    "            # operations\n",
    "            # x inputs\n",
    "\n",
    "            y = target_cl.sel(longitude = long, latitude = lat).sel(forecast_time = slice('2000-01-01', '2019-12-30'))\n",
    "            yy = y.to_dataframe().reset_index()\n",
    "            yy_inp = yy.drop(['latitude','longitude', 'lead_time'], axis = 'columns') # valid_time is not found in the case of using target_class\n",
    "            yy_inpVar = yy_inp[['tp']]\n",
    "\n",
    "            yy_inpVar_train = yy_inpVar.loc[0:900]\n",
    "            y_train = yy_inpVar_train.values.ravel()\n",
    "\n",
    "            yy_inpVar_test = yy_inpVar.loc[901:]\n",
    "            y_test = yy_inpVar_test.values.ravel()\n",
    "\n",
    "            for r in range(0,11): \n",
    "\n",
    "                # training and testing data\n",
    "\n",
    "                x = hecm_int.isel(realization = r).sel(forecast_time = slice('2000-01-01', '2019-12-30'))\n",
    "                xx = x.to_dataframe().reset_index()\n",
    "                xx_inp = xx.drop(['latitude','longitude', 'lead_time', 'valid_time'], axis = 'columns')\n",
    "                xx_inpVar = xx_inp[['tp','t2m']]\n",
    "\n",
    "                xx_inpVar_train = xx_inpVar.loc[0:900]\n",
    "                x_train = xx_inpVar_train.values\n",
    "\n",
    "                xx_inpVar_test = xx_inpVar.loc[901:]\n",
    "                x_test = xx_inpVar_test.values\n",
    "\n",
    "                # Training the LR and MLP models\n",
    "\n",
    "                min_max_scaler = preprocessing.MinMaxScaler()\n",
    "                x_trainT = min_max_scaler.fit_transform(x_train)\n",
    "                x_testT = min_max_scaler.fit_transform(x_test)\n",
    "\n",
    "                if  model == 'LR':\n",
    "                    lr = LogisticRegression()\n",
    "                    lr_model = lr.fit(x_trainT, y_train)\n",
    "                    training_accuracy = lr_model.score(x_train, y_train)\n",
    "    #                 print(training_accuracy)\n",
    "\n",
    "                if model == 'MLP':\n",
    "\n",
    "                    # define model\n",
    "                    mlp_model = Sequential()\n",
    "                    mlp_model.add(Dense(500, input_dim=len(xx_inpVar_train.columns), activation='relu'))\n",
    "                    mlp_model.add(Dense(3, activation='softmax'))\n",
    "                    mlp_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "                    # simple early stopping\n",
    "                    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "                    mc = ModelCheckpoint('models/'+ region +'_ct/model_' + str(r + 1) + '_' +  str(i) + str(j) + '.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "                    # fit model\n",
    "                    Model = mlp_model.fit(x_trainT, y_train, validation_data=(x_testT[0:106], y_test[0:106]), epochs=4000, verbose=0, callbacks=[es, mc])        \n",
    "\n",
    "                    # load the model\n",
    "                    Model = load_model('models/' + region + '_ct/model_' + str(r + 1) + '_' +  str(i) + str(j) + '.h5')\n",
    "\n",
    "                    # make prediction\n",
    "                    yhat = Model.predict(x_testT, verbose=0)\n",
    "\n",
    "                    # stack predictions into [rows, members, probabilities]\n",
    "                    if stackX is None:\n",
    "                        stackX = yhat\n",
    "                    else:\n",
    "                        stackX = dstack((stackX, yhat))\n",
    "\n",
    "            stackXX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "\n",
    "            meta_model = LogisticRegression()\n",
    "\n",
    "            meta_model.fit(stackXX[0:106], y_test[0:106])\n",
    "\n",
    "            training_accuracy = meta_model.score(stackXX[0:106], y_test[0:106])\n",
    "\n",
    "            yy_pred = meta_model.predict_proba(stackXX[106:])\n",
    "\n",
    "            accuarcy = meta_model.score(stackXX[106:], y_test[106:])\n",
    "\n",
    "            # save the meta_model\n",
    "            import pickle\n",
    "            pickle.dump(meta_model, open('models/' + region + '_ct/meta_model.sav', 'wb'))\n",
    "\n",
    "            # add the values to the empty array\n",
    "\n",
    "            prediction_an[:, j, i] = yy_pred[:,2]\n",
    "            prediction_nn[:, j, i] = yy_pred[:,1]\n",
    "            prediction_bn[:, j, i] = yy_pred[:,0]\n",
    "\n",
    "            testing[:, j, i] = y_test[106:]\n",
    "            training_acc[0, j, i] = training_accuracy \n",
    "            testing_acc[0, j, i] = accuracy   \n",
    "\n",
    "            j = j + 1 \n",
    "\n",
    "            print(1)\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    # Make an xarray of predictions data\n",
    "\n",
    "    time_step = [i for i in range(0, len(yy_pred))]\n",
    "\n",
    "    longitude = longitude.values\n",
    "    latitude = latitude.values\n",
    "\n",
    "\n",
    "    predict_an = xr.DataArray(\n",
    "    data=prediction_an,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "    longitude=([\"longitude\"], longitude),\n",
    "    latitude=([\"latitude\"], latitude),      \n",
    "    time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "    description=\"predictions_an\",\n",
    "    ),\n",
    "    )\n",
    "\n",
    "    predict_nn = xr.DataArray(\n",
    "    data=prediction_nn,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "    longitude=([\"longitude\"], longitude),\n",
    "    latitude=([\"latitude\"], latitude),      \n",
    "    time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "    description=\"predictions_nn\",\n",
    "    ),\n",
    "    )\n",
    "\n",
    "    predict_bn = xr.DataArray(\n",
    "    data=prediction_bn,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "    longitude=([\"longitude\"], longitude),\n",
    "    latitude=([\"latitude\"], latitude),      \n",
    "    time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "    description=\"predictions_bn\",\n",
    "    ),\n",
    "    )\n",
    "\n",
    "    # make categories from the predictions\n",
    "\n",
    "    categories = xr.concat([predict_bn, predict_nn, predict_an],'category').assign_coords(category=['below normal', 'near normal', 'above normal'])\n",
    "    categorical_p = categories.rename({'time_step':'forecast_time'})\n",
    "\n",
    "    # Make an xarray of tests data\n",
    "\n",
    "    test = xr.DataArray(\n",
    "        data=testing,\n",
    "        dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "            time_step= ([\"time_step\"], time_step)\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"predictions\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Make an xarray for the testing accuracies\n",
    "\n",
    "    test_acc = xr.DataArray(\n",
    "        data=testing_acc,\n",
    "        dims=[\"testing_accuracy\", \"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"testing accuracy\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Make an xarray for the training accuracies\n",
    "\n",
    "    train_acc = xr.DataArray(\n",
    "        data=training_acc,\n",
    "        dims=[\"training_accuracy\", \"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"testing accuracy\",\n",
    "        ),\n",
    "    )\n",
    "    return test_acc, train_acc, categorical_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, train_acc, categorical_p = CMcor('region1', 'one' , 'MLP', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_p = hindcast_observprob['tp'].isel(lead_time = 0).sel(forecast_time = slice('2018-01-01', '2019-12-30'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_p['forecast_time'] = obs_p['forecast_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RPSS_cl, RPSS_ecmwf = RPSS12('region3', categorical_p, 0, '2018-01-01', '2019-12-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region1 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 100, 36, 'tp', 0, '2018-01-01', '2019-12-30')\n",
    "# region2 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 100, 55, 'tp', 0, '2018-01-01', '2019-12-30')\n",
    "region3 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 6, 53, 'tp', 0, '2018-01-01', '2019-12-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function of the stacked model\n",
    "# Adding the correlated inputs\n",
    "\n",
    "def CM2cv1tecor(region, spatial, model, lead_time):\n",
    "    \n",
    "    # import the data\n",
    "\n",
    "    # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "    hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).chunk(dict(forecast_time=-1))\n",
    "    hobs = hindcast_observation.sel(forecast_time = slice('2000-01-01', '2012-12-30')).isel(lead_time  = 0).chunk(dict(forecast_time=-1))\n",
    "    hobs_int = hobs.interpolate_na(dim = 'forecast_time')\n",
    "    hecm_interpolate = hecm.interpolate_na(dim = 'forecast_time')\n",
    "\n",
    "    target_cl = target_class.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).fillna(10).chunk(dict(forecast_time=-1))\n",
    "\n",
    "    # hecm_int['cl'] = target_cl \n",
    "    # hobs_int['cl'] = target_cl\n",
    "\n",
    "    # modify the longitude and latitude to the selected regions:\n",
    "\n",
    "    # regions\n",
    "\n",
    "    region1 = hobs.sel(longitude = slice(95, 100)).sel(latitude = slice(36, 31))\n",
    "    region2 = hobs.sel(longitude = slice(100, 105)).sel(latitude = slice(60, 55))\n",
    "    region3 = hobs.sel(longitude = slice(5, 10)).sel(latitude = slice(53, 48))\n",
    "\n",
    "    if region == 'region1':\n",
    "        longitude = region1.longitude\n",
    "        latitude  = region1.latitude\n",
    "\n",
    "    if region == 'region2':\n",
    "        longitude = region2.longitude\n",
    "        latitude  = region2.latitude\n",
    "\n",
    "    if region == 'region3':\n",
    "        longitude = region3.longitude\n",
    "        latitude  = region3.latitude\n",
    "\n",
    "    # create the empty arrays for the results\n",
    "    x = hecm_interpolate.mean(dim = 'realization').sel(forecast_time = slice('2019-01-01', '2019-12-30'))\n",
    "\n",
    "    prediction_an = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "    prediction_nn = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "    prediction_bn = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "\n",
    "    testing = np.zeros((len(x.forecast_time), len(latitude), len(longitude)))\n",
    "\n",
    "    training_acc = np.zeros((1, len(latitude), len(longitude)))\n",
    "    testing_acc = np.zeros((1, len(latitude), len(longitude)))\n",
    "\n",
    "    i = 0\n",
    "    for long in longitude:\n",
    "\n",
    "        j = 0\n",
    "        for lat in latitude:\n",
    "\n",
    "            stackX = None\n",
    "\n",
    "            # choose the input and output data\n",
    "\n",
    "            # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "            hecm_cell = hecm.sel(longitude = long, latitude = lat)\n",
    "            hobs_cell = hobs.sel(longitude = long, latitude = lat)\n",
    "            hobs_int = hobs_cell.interpolate_na(dim = 'forecast_time')\n",
    "            hecm_int = hecm_cell.interpolate_na(dim = 'forecast_time')\n",
    "            # hobs_int['cl'] = target_cl\n",
    "\n",
    "            # operations\n",
    "            # adding the target outputs for training and testing\n",
    "\n",
    "            y = target_cl.sel(longitude = long, latitude = lat).sel(forecast_time = slice('2000-01-01', '2019-12-30'))\n",
    "            yy = y.to_dataframe().reset_index()\n",
    "            yy_inp = yy.drop(['latitude','longitude', 'lead_time'], axis = 'columns') # valid_time is not found in the case of using target_class\n",
    "            yy_inpVar = yy_inp[['tp']]\n",
    "\n",
    "            yy_inpVar_train = yy_inpVar.loc[0:900]\n",
    "            y_train = yy_inpVar_train.values.ravel()\n",
    "\n",
    "            yy_inpVar_test = yy_inpVar.loc[901:]\n",
    "            y_test = yy_inpVar_test.values.ravel()\n",
    "\n",
    "            for r in range(0,11): \n",
    "\n",
    "                # add the correlated inputs\n",
    "                x = hecm_int.isel(realization = r).sel(forecast_time = slice('2000-01-01', '2019-12-30'))\n",
    "                xx = x.to_dataframe().reset_index()\n",
    "                xx['tp_lag1'] = xx.tp.shift(1).bfill()\n",
    "\n",
    "                if spatial == 'one':\n",
    "                    disla = lat - (1 * 1.5)\n",
    "                    dislo = long - (1 * 1.5)\n",
    "                    dla = np.arange(0, 4.5, 1.5)\n",
    "                    dlo = np.arange(0, 4.5, 1.5)\n",
    "                    v = 'var 4'\n",
    "\n",
    "                if spatial == 'two':\n",
    "                    disla = lat - (2 * 1.5)\n",
    "                    dislo = long - (2 * 1.5)\n",
    "                    dla = np.arange(0, 7.5, 1.5)\n",
    "                    dlo = np.arange(0, 7.5, 1.5)\n",
    "                    v = 'var 12'\n",
    "\n",
    "                if spatial == 'three':\n",
    "                    disla = lat - (3 * 1.5)\n",
    "                    dislo = long - (3 * 1.5)\n",
    "                    dla = np.arange(0, 10.5, 1.5)\n",
    "                    dlo = np.arange(0, 10.5, 1.5)\n",
    "                    v = 'var 24'\n",
    "\n",
    "                # var = []\n",
    "                nam = 0\n",
    "                for ii in dlo:\n",
    "                    for jj in dla:\n",
    "\n",
    "                        xx['Var'] = hecm_interpolate.tp.sel(latitude = disla + jj, longitude = dislo + ii).sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(realization = r)\n",
    "                        name = 'var {}'.format(nam)\n",
    "                        xx.rename(columns={'Var': name}, inplace=True)\n",
    "                        nam = nam + 1\n",
    "\n",
    "\n",
    "                xx_dropna = xx.dropna(how='all', axis=1)\n",
    "                xx_inp_train = xx_dropna.loc[0:900]\n",
    "                xx_inp_tr = xx_inp_train.drop(['forecast_time','latitude','longitude', 'lead_time', 'valid_time', 'realization'], axis = 'columns')\n",
    "                xx_inpVar_train = xx_inp_tr.drop([v], axis = 'columns')\n",
    "\n",
    "                x_train = xx_inpVar_train.values\n",
    "                print(x_train.shape)\n",
    "\n",
    "                # testing data\n",
    "                xx_inp_test = xx_dropna.loc[901:]\n",
    "                xx_inp_te = xx_inp_test.drop(['forecast_time','latitude','longitude', 'lead_time', 'valid_time', 'realization'], axis = 'columns')\n",
    "                xx_inpVar_test = xx_inp_te.drop([v], axis = 'columns')\n",
    "\n",
    "                x_test = xx_inpVar_test.values\n",
    "\n",
    "                # Training the LR and MLP models\n",
    "\n",
    "                min_max_scaler = preprocessing.MinMaxScaler()\n",
    "                x_trainT = min_max_scaler.fit_transform(x_train)\n",
    "                x_testT = min_max_scaler.fit_transform(x_test)\n",
    "\n",
    "                if  model == 'LR':\n",
    "                    lr = LogisticRegression()\n",
    "                    lr_model = lr.fit(x_trainT, y_train)\n",
    "                    training_accuracy = lr_model.score(x_train, y_train)\n",
    "    #                 print(training_accuracy)\n",
    "\n",
    "                if model == 'MLP':\n",
    "\n",
    "                    # define model\n",
    "                    mlp_model = Sequential()\n",
    "                    mlp_model.add(Dense(500, input_dim=len(xx_inpVar_train.columns), activation='relu'))\n",
    "                    mlp_model.add(Dense(3, activation='softmax'))\n",
    "                    mlp_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "                    # simple early stopping\n",
    "                    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "                    mc = ModelCheckpoint('models/'+ region + spatial + '_ct/model_' + str(lead_time) + str(r + 1) + '_' +  str(i) + str(j) + '.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "                    # fit model\n",
    "                    Model = mlp_model.fit(x_trainT, y_train, validation_data=(x_testT[0:106], y_test), epochs=4000, verbose=0, callbacks=[es, mc])        \n",
    "\n",
    "                    # load the model\n",
    "                    Model = load_model('models/' + region + spatial + '_ct/model_' + str(lead_time) + str(r + 1) + '_' +  str(i) + str(j) + '.h5')\n",
    "\n",
    "                    # make prediction\n",
    "                    yhat = Model.predict(x_testT, verbose=0)\n",
    "\n",
    "                    # stack predictions into [rows, members, probabilities]\n",
    "                    if stackX is None:\n",
    "                        stackX = yhat\n",
    "                    else:\n",
    "                        stackX = dstack((stackX, yhat))\n",
    "\n",
    "            stackXX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "\n",
    "            stackXX = np.nan_to_num(stackXX)\n",
    "\n",
    "            meta_model = LogisticRegression()\n",
    "\n",
    "            meta_model.fit(stackXX[0:106], y_test[0:106])\n",
    "\n",
    "            training_accuracy = meta_model.score(stackXX[0:106], y_test[0:106])\n",
    "            accuracy = training_accuracy\n",
    "\n",
    "            yy_pred = meta_model.predict_proba(stackXX[106:])\n",
    "\n",
    "            # save the meta_model\n",
    "            pickle.dump(meta_model, open('models/' + region + spatial + '_ct/meta_model_' + str(lead_time) + '_' +  str(i) + str(j) + '.sav', 'wb'))\n",
    "\n",
    "            # add the values to the empty array\n",
    "\n",
    "            prediction_an[:, j, i] = yy_pred[:,2]\n",
    "            prediction_nn[:, j, i] = yy_pred[:,1]\n",
    "            prediction_bn[:, j, i] = yy_pred[:,0]\n",
    "\n",
    "            testing[:, j, i] = y_test[106:]\n",
    "            training_acc[0, j, i] = training_accuracy \n",
    "            testing_acc[0, j, i] = accuracy   \n",
    "\n",
    "            j = j + 1 \n",
    "\n",
    "            print(1)\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    # Make an xarray of predictions data\n",
    "\n",
    "    time_step = [i for i in range(0, len(yy_pred))]\n",
    "\n",
    "    longitude = longitude.values\n",
    "    latitude = latitude.values\n",
    "\n",
    "\n",
    "    predict_an = xr.DataArray(\n",
    "    data=prediction_an,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "    longitude=([\"longitude\"], longitude),\n",
    "    latitude=([\"latitude\"], latitude),      \n",
    "    time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "    description=\"predictions_an\",\n",
    "    ),\n",
    "    )\n",
    "\n",
    "    predict_nn = xr.DataArray(\n",
    "    data=prediction_nn,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "    longitude=([\"longitude\"], longitude),\n",
    "    latitude=([\"latitude\"], latitude),      \n",
    "    time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "    description=\"predictions_nn\",\n",
    "    ),\n",
    "    )\n",
    "\n",
    "    predict_bn = xr.DataArray(\n",
    "    data=prediction_bn,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "    longitude=([\"longitude\"], longitude),\n",
    "    latitude=([\"latitude\"], latitude),      \n",
    "    time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "    description=\"predictions_bn\",\n",
    "    ),\n",
    "    )\n",
    "\n",
    "    # make categories from the predictions\n",
    "\n",
    "    categories = xr.concat([predict_bn, predict_nn, predict_an],'category').assign_coords(category=['below normal', 'near normal', 'above normal'])\n",
    "    categorical_p = categories.rename({'time_step':'forecast_time'})\n",
    "\n",
    "    # Make an xarray of tests data\n",
    "\n",
    "    test = xr.DataArray(\n",
    "        data=testing,\n",
    "        dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "            time_step= ([\"time_step\"], time_step)\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"predictions\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Make an xarray for the testing accuracies\n",
    "\n",
    "    test_acc = xr.DataArray(\n",
    "        data=testing_acc,\n",
    "        dims=[\"testing_accuracy\", \"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"testing accuracy\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Make an xarray for the training accuracies\n",
    "\n",
    "    train_acc = xr.DataArray(\n",
    "        data=training_acc,\n",
    "        dims=[\"training_accuracy\", \"latitude\", \"longitude\"],\n",
    "        coords=dict(\n",
    "            longitude=([\"longitude\"], longitude),\n",
    "            latitude=([\"latitude\"], latitude),       \n",
    "        ),\n",
    "        attrs=dict(\n",
    "            description=\"testing accuracy\",\n",
    "        ),\n",
    "    )\n",
    "    return test_acc, train_acc, categorical_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, train_acc, categorical_p = CM2cv1tecor('region1', 'one' , 'MLP', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_p = hindcast_observprob['tp'].isel(lead_time = 0).sel(forecast_time = slice('2018-01-01', '2019-12-30'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_p['forecast_time'] = obs_p['forecast_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RPSS_cl, RPSS_ecmwf = RPSS12('region3', categorical_p, 0, '2018-01-01', '2019-12-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region1 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 100, 36, 'tp', 0, '2018-01-01', '2019-12-30')\n",
    "# region2 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 100, 55, 'tp', 0, '2018-01-01', '2019-12-30')\n",
    "region3 = ROC_ML(hindcast_observation , categorical_p, tercile_edges, 6, 53, 'tp', 0, '2018-01-01', '2019-12-30')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the CM using the real-time forecasting of 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the classes from the CPC observation \n",
    "\n",
    "forecast_terTR = ter_forecast.assign_coords(category=[0, 1, 2])\n",
    "classes = (forecast_terTR * forecast_terTR.category).sum('category')\n",
    "class_forecast = classes.chunk(dict(forecast_time=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the saved models.\n",
    "# Using the real-time forecasting of 2020 as input.\n",
    "\n",
    "def CMte2020(region, model, lead_time):\n",
    "\n",
    "    # import the data\n",
    "\n",
    "    # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "    fecm = forecast_ecmwf.isel(lead_time = 0).chunk(dict(forecast_time=-1))\n",
    "    hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).chunk(dict(forecast_time=-1))\n",
    "    hobs = hindcast_observation.sel(forecast_time = slice('2000-01-01', '2012-12-30')).isel(lead_time  = 0).chunk(dict(forecast_time=-1))\n",
    "    hobs_int = hobs.interpolate_na(dim = 'forecast_time')\n",
    "    hecm_int = hecm.interpolate_na(dim = 'forecast_time')\n",
    "    fecm_interpolate = fecm.interpolate_na(dim = 'forecast_time')\n",
    "\n",
    "\n",
    "    # modify the longitude and latitude to the selected regions:\n",
    "\n",
    "    # regions\n",
    "\n",
    "    region1 = hobs.sel(longitude = slice(95, 100)).sel(latitude = slice(36, 31))\n",
    "    region2 = hobs.sel(longitude = slice(100, 105)).sel(latitude = slice(60, 55))\n",
    "    region3 = hobs.sel(longitude = slice(5, 10)).sel(latitude = slice(53, 48))\n",
    "\n",
    "    if region == 'region1':\n",
    "        longitude = region1.longitude\n",
    "        latitude  = region1.latitude\n",
    "\n",
    "    if region == 'region2':\n",
    "        longitude = region2.longitude\n",
    "        latitude  = region2.latitude\n",
    "\n",
    "    if region == 'region3':\n",
    "        longitude = region3.longitude\n",
    "        latitude  = region3.latitude\n",
    "\n",
    "    # create the empty arrays for the results\n",
    "    x = hecm_int.mean(dim = 'realization').sel(forecast_time = slice('2018-01-01', '2019-12-30'))\n",
    "\n",
    "    prediction_an = np.zeros((53, len(latitude), len(longitude)))\n",
    "    prediction_nn = np.zeros((53, len(latitude), len(longitude)))\n",
    "    prediction_bn = np.zeros((53, len(latitude), len(longitude)))\n",
    "\n",
    "    i = 0\n",
    "    for long in longitude:\n",
    "\n",
    "        j = 0\n",
    "        for lat in latitude:\n",
    "\n",
    "            stackX = None\n",
    "\n",
    "            # choose the input and output data\n",
    "\n",
    "            # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "            hecm_cell = hecm.sel(longitude = long, latitude = lat)\n",
    "            hobs_cell = hobs.sel(longitude = long, latitude = lat)\n",
    "            hobs_int = hobs_cell.interpolate_na(dim = 'forecast_time')\n",
    "            hecm_int = hecm_cell.interpolate_na(dim = 'forecast_time')\n",
    "\n",
    "            fecm_cell = fecm.sel(longitude = long, latitude = lat)\n",
    "            fecm_int = fecm_cell.interpolate_na(dim = 'forecast_time')\n",
    "            # hobs_int['cl'] = target_cl\n",
    "\n",
    "            # operations\n",
    "            # x inputs\n",
    "\n",
    "            y = class_forecast.isel(lead_time = 0).sel(longitude = long, latitude = lat)\n",
    "            yy = y.to_dataframe().reset_index()\n",
    "            yy_inp = yy.drop(['latitude','longitude', 'lead_time'], axis = 'columns') # valid_time is not found in the case of using target_class\n",
    "            yy_inpVar = yy_inp[['tp']]\n",
    "\n",
    "            y_test = yy_inpVar.values.ravel()\n",
    "\n",
    "            m = 1\n",
    "            for r in range(0,11): \n",
    "\n",
    "                # training and testing data\n",
    "                x = fecm_int.isel(realization = m)\n",
    "                xx = x.to_dataframe().reset_index()\n",
    "\n",
    "                xx_inp = xx.drop(['latitude','longitude', 'lead_time', 'valid_time'], axis = 'columns')\n",
    "                xx_inpVar = xx_inp[['tp','t2m']]\n",
    "\n",
    "                x_test = xx_inpVar.values\n",
    "\n",
    "                # Training the LR and MLP models\n",
    "\n",
    "                min_max_scaler = preprocessing.MinMaxScaler()\n",
    "                x_testT = min_max_scaler.fit_transform(x_test)\n",
    "\n",
    "                m = m + 1\n",
    "    #             if r == 9:\n",
    "    #                 m = 50\n",
    "\n",
    "                if  model == 'LR':\n",
    "                    lr = LogisticRegression()\n",
    "                    lr_model = lr.fit(x_trainT, y_train)\n",
    "                    training_accuracy = lr_model.score(x_train, y_train)\n",
    "    #                 print(training_accuracy)\n",
    "\n",
    "                if model == 'MLP':\n",
    "\n",
    "                    # load the model\n",
    "                    Model = load_model('models/'+ region +'/model_' + str(lead_time) + str(r + 1) + '_' +  str(i) + str(j) + '.h5')\n",
    "\n",
    "                    # make prediction\n",
    "                    yhat = Model.predict(x_testT, verbose=0)\n",
    "\n",
    "                    # stack predictions into [rows, members, probabilities]\n",
    "                    if stackX is None:\n",
    "                        stackX = yhat\n",
    "                    else:\n",
    "                        stackX = dstack((stackX, yhat))\n",
    "\n",
    "            stackXX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "            stackXX = np.nan_to_num(stackXX)\n",
    "            # Load the meta model\n",
    "\n",
    "            meta_model = pickle.load(open('models/' + region + '/meta_model_' + str(lead_time) + '_' +  str(i) + str(j) + '.sav', 'rb'))\n",
    "\n",
    "            yy_pred = meta_model.predict_proba(stackXX)\n",
    "\n",
    "            # add the values to the empty array\n",
    "\n",
    "            prediction_an[:, j, i] = yy_pred[:,2]\n",
    "            prediction_nn[:, j, i] = yy_pred[:,1]\n",
    "            prediction_bn[:, j, i] = yy_pred[:,0]\n",
    "\n",
    "            j = j + 1 \n",
    "\n",
    "            print(1)\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    # Make an xarray of predictions data\n",
    "\n",
    "    time_step = [i for i in range(0, len(yy_pred))]\n",
    "\n",
    "    longitude = longitude.values\n",
    "    latitude = latitude.values\n",
    "\n",
    "\n",
    "    predict_an = xr.DataArray(\n",
    "    data=prediction_an,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "    longitude=([\"longitude\"], longitude),\n",
    "    latitude=([\"latitude\"], latitude),      \n",
    "    time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "    description=\"predictions_an\",\n",
    "    ),\n",
    "    )\n",
    "\n",
    "    predict_nn = xr.DataArray(\n",
    "    data=prediction_nn,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "    longitude=([\"longitude\"], longitude),\n",
    "    latitude=([\"latitude\"], latitude),      \n",
    "    time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "    description=\"predictions_nn\",\n",
    "    ),\n",
    "    )\n",
    "\n",
    "    predict_bn = xr.DataArray(\n",
    "    data=prediction_bn,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "    longitude=([\"longitude\"], longitude),\n",
    "    latitude=([\"latitude\"], latitude),      \n",
    "    time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "    description=\"predictions_bn\",\n",
    "    ),\n",
    "    )\n",
    "\n",
    "    # make categories from the predictions\n",
    "\n",
    "    categories = xr.concat([predict_bn, predict_nn, predict_an],'category').assign_coords(category=['below normal', 'near normal', 'above normal'])\n",
    "    categorical_p = categories.rename({'time_step':'forecast_time'})\n",
    "    \n",
    "    return categorical_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_p = CMte2020('region1','MLP', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPSS and ROC curve for the  testing of the 2020 real-time forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xskillscore as xs\n",
    "from scripts import  make_probabilistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_observprob = make_probabilistic(forecast_observation, tercile_edges = tercile_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del forecast_observation ['week']\n",
    "del forecast_observation ['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast_observprob.to_netcdf('D:/Downloads/forecast_observprob.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_p = forecast_observprob['tp'].isel(lead_time = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_p = xr.DataArray([1/3, 1/3, 1/3], dims='category', coords={'category':['below normal', 'near normal', 'above normal']}).to_dataset(name='tp')\n",
    "clim_p['t2m'] = clim_p['tp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_p['forecast_time'] = obs_p['forecast_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region1 = rpss_cl.sel(longitude = slice(95, 100)).sel(latitude = slice(36, 31))\n",
    "# region2 = rpss_cl.sel(longitude = slice(100, 105)).sel(latitude = slice(60, 55))\n",
    "# region3 = rpss_cl.sel(longitude = slice(5, 10)).sel(latitude = slice(53, 48))\n",
    "\n",
    "def RPSS_2020(region, categorical, lead_time, start, end):\n",
    "        \n",
    "    # regions\n",
    "\n",
    "    region1 = hindcast_observation.sel(longitude = slice(95, 100)).sel(latitude = slice(36, 31))\n",
    "    region2 = hindcast_observation.sel(longitude = slice(100, 105)).sel(latitude = slice(60, 55))\n",
    "    region3 = hindcast_observation.sel(longitude = slice(5, 10)).sel(latitude = slice(53, 48))\n",
    "\n",
    "    if region == 'region1':\n",
    "        long = region1.longitude\n",
    "        lat  = region1.latitude\n",
    "        \n",
    "    if region == 'region2':\n",
    "        long = region2.longitude\n",
    "        lat  = region2.latitude\n",
    "    \n",
    "    if region == 'region3':\n",
    "        long = region3.longitude\n",
    "        lat  = region3.latitude\n",
    "    \n",
    "    obs_p = forecast_observprob['tp'].isel(lead_time = lead_time).sel(forecast_time = slice(start, end)).sel(longitude = long).sel(latitude = lat)\n",
    "    \n",
    "    rps_cl = xs.rps(obs_p, clim_p['tp'], category_edges=None, dim=[], input_distributions='p').compute()\n",
    "    rps_model = xs.rps(obs_p, categorical, category_edges=None, dim=[], input_distributions='p').compute()\n",
    "    \n",
    "    rpss_cl = 1 - (rps_model.mean('forecast_time') / rps_cl.mean('forecast_time'))\n",
    "    \n",
    "    RPSS_cl = rpss_cl.mean(dim = 'latitude').mean(dim = 'longitude').values\n",
    "\n",
    "    print(RPSS_cl)\n",
    "    \n",
    "    return RPSS_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RPSS_cl = RPSS_2020('region3', categorical_p, 0, '2020-01-02', '2020-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_ML2020(dataset, probabilistic, tercile_edges, longitude, latitude, variable, lead_time, start, end):\n",
    "    aprob = probabilistic.sel(forecast_time= slice(start, end)).sel(longitude = longitude, latitude = latitude, method = 'nearest').isel(category = 2).values\n",
    "    nnprob = probabilistic.sel(forecast_time= slice(start, end)).sel(longitude = longitude, latitude = latitude, method = 'nearest').isel(category = 1).values\n",
    "    bprob = probabilistic.sel(forecast_time= slice(start, end)).sel(longitude = longitude, latitude = latitude, method = 'nearest').isel(category = 0).values\n",
    "    \n",
    "    x_plus = dataset[variable].sel(forecast_time= slice(start, end)).isel(lead_time = lead_time).sel(longitude = longitude, latitude = latitude, method = 'nearest')\n",
    "\n",
    "    upper = []\n",
    "    lower = []\n",
    "    for i in range(0,len(dataset.forecast_time)):\n",
    "        if i < 53:\n",
    "            u = tercile_edges[variable].isel(week = i, category_edge = 1, lead_time = lead_time).sel(longitude = longitude, latitude = latitude, method = 'nearest').values\n",
    "            upper.append(u)\n",
    "            l = tercile_edges[variable].isel(week = i, category_edge = 0, lead_time = lead_time).sel(longitude = longitude, latitude = latitude, method = 'nearest').values\n",
    "            lower.append(l)\n",
    "\n",
    "    for i in range(0,(int(len(dataset.forecast_time)/53)-1)):\n",
    "        for j in range(0, 53):\n",
    "            upper.append(upper[j])\n",
    "            lower.append(lower[j])\n",
    "            \n",
    "    dataset['upper']=(['forecast_time'], upper)\n",
    "    dataset['lower']=(['forecast_time'], lower)\n",
    "\n",
    "    u = dataset.upper.sel(forecast_time= slice(start, end))\n",
    "\n",
    "    l = dataset.lower.sel(forecast_time= slice(start, end))\n",
    "\n",
    "    \n",
    "    forecast_time = x_plus.forecast_time\n",
    "    \n",
    "    above = []\n",
    "    nnormal = []\n",
    "    below = []\n",
    "    \n",
    "    \n",
    "    for i in range(0, len(forecast_time)):\n",
    "        \n",
    "        if x_plus.isel(forecast_time = i).values > u.isel(forecast_time = i):\n",
    "            a = 1\n",
    "            above.append(a)\n",
    "        else:\n",
    "            a = 0\n",
    "            above.append(a)\n",
    "            \n",
    "        if l.isel(forecast_time = i) < x_plus.isel(forecast_time =  i) < u.isel(forecast_time = i):\n",
    "            nn = 1\n",
    "            nnormal.append(nn)\n",
    "        else:\n",
    "            nn = 0\n",
    "            nnormal.append(nn)\n",
    "        \n",
    "        if x_plus.isel(forecast_time = i) < l.isel(forecast_time = i):\n",
    "\n",
    "            b = 1\n",
    "            below.append(b)\n",
    "        else:\n",
    "            b = 0\n",
    "            below.append(b)\n",
    "    \n",
    "    \n",
    "    ns_probs = [0 for _ in range(len(aprob))]\n",
    "    # calculate scores\n",
    "    ns_auc = roc_auc_score(above, ns_probs)\n",
    "    nma_auc = roc_auc_score(above, aprob)\n",
    "    nmnn_auc = roc_auc_score(nnormal, nnprob)\n",
    "    nmb_auc = roc_auc_score(below, bprob)\n",
    "    \n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(above, ns_probs)\n",
    "    nma_fpr, nma_tpr, _ = roc_curve(above, aprob)\n",
    "    nmnn_fpr, nmnn_tpr, _ = roc_curve(nnormal, nnprob)\n",
    "    nmb_fpr, nmb_tpr, _ = roc_curve(below, bprob)\n",
    "    \n",
    "    # plot the ROC\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n",
    "    axes.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "    axes.plot(nma_fpr, nma_tpr, color = 'r', label = 'Above normal, AUC = {:.3f}'.format(nma_auc))\n",
    "    axes.plot(nmnn_fpr, nmnn_tpr, color = 'g', label = 'Near normal, AUC = {:.3f}'.format(nmnn_auc))\n",
    "    axes.plot(nmb_fpr, nmb_tpr, color = 'y', label = 'Below normal, AUC = {:.3f}'.format(nmb_auc))\n",
    "    axes.set_ylim(ymin = 0)\n",
    "    axes.set_xlim(xmin = 0)\n",
    "    axes.set(xlabel= 'False Positive Rate', ylabel='True Positive Rate', title = 'Receiver operating characteristic curve (ROC)')\n",
    "    axes.legend(loc=\"upper left\")\n",
    "    \n",
    "    del dataset['upper']\n",
    "    del dataset['lower']\n",
    "    \n",
    "    return aprob, nnprob, bprob, above, nnormal, below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region1 = ROC_ML2020(forecast_observation , categorical_p, tercile_edges, 100, 36, 'tp', 0, '2020-01-02', '2020-12-31')\n",
    "# region2 = ROC_ML2020(forecast_observation , categorical_p, tercile_edges, 100, 55, 'tp', 0, '2020-01-02', '2020-12-31')\n",
    "NNl_roc3 = ROC_ML2020(forecast_observation , categorical_p, tercile_edges, 6, 53, 'tp', 0, '2020-01-02', '2020-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the saved models.\n",
    "# Using the real-time forecasting of 2020 as input.\n",
    "# adding the correlated inputs.\n",
    "\n",
    "def CMte2020cor(region, spatial, model, lead_time):\n",
    "\n",
    "    # import the data\n",
    "\n",
    "    # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "    fecm = forecast_ecmwf.isel(lead_time = 0).chunk(dict(forecast_time=-1))\n",
    "    hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).chunk(dict(forecast_time=-1))\n",
    "    hobs = hindcast_observation.sel(forecast_time = slice('2000-01-01', '2012-12-30')).isel(lead_time  = 0).chunk(dict(forecast_time=-1))\n",
    "    hobs_int = hobs.interpolate_na(dim = 'forecast_time')\n",
    "    hecm_int = hecm.interpolate_na(dim = 'forecast_time')\n",
    "    fecm_interpolate = fecm.interpolate_na(dim = 'forecast_time')\n",
    "\n",
    "    # modify the longitude and latitude to the selected regions:\n",
    "\n",
    "    # regions\n",
    "\n",
    "    region1 = hobs.sel(longitude = slice(95, 100)).sel(latitude = slice(36, 31))\n",
    "    region2 = hobs.sel(longitude = slice(100, 105)).sel(latitude = slice(60, 55))\n",
    "    region3 = hobs.sel(longitude = slice(5, 10)).sel(latitude = slice(53, 48))\n",
    "\n",
    "    if region == 'region1':\n",
    "        longitude = region1.longitude\n",
    "        latitude  = region1.latitude\n",
    "\n",
    "    if region == 'region2':\n",
    "        longitude = region2.longitude\n",
    "        latitude  = region2.latitude\n",
    "\n",
    "    if region == 'region3':\n",
    "        longitude = region3.longitude\n",
    "        latitude  = region3.latitude\n",
    "\n",
    "    # create the empty arrays for the results\n",
    "    x = hecm_int.mean(dim = 'realization').sel(forecast_time = slice('2018-01-01', '2019-12-30'))\n",
    "\n",
    "    prediction_an = np.zeros((53, len(latitude), len(longitude)))\n",
    "    prediction_nn = np.zeros((53, len(latitude), len(longitude)))\n",
    "    prediction_bn = np.zeros((53, len(latitude), len(longitude)))\n",
    "\n",
    "    i = 0\n",
    "    for long in longitude:\n",
    "\n",
    "        j = 0\n",
    "        for lat in latitude:\n",
    "\n",
    "            stackX = None\n",
    "\n",
    "            # choose the input and output data\n",
    "\n",
    "            # hecm = hindcast_ecmwf.sel(forecast_time = slice('2000-01-01', '2019-12-30')).isel(lead_time = 0).mean(dim = 'realization')\n",
    "            hecm_cell = hecm.sel(longitude = long, latitude = lat)\n",
    "            hobs_cell = hobs.sel(longitude = long, latitude = lat)\n",
    "            hobs_int = hobs_cell.interpolate_na(dim = 'forecast_time')\n",
    "            hecm_int = hecm_cell.interpolate_na(dim = 'forecast_time')\n",
    "\n",
    "            fecm_cell = fecm.sel(longitude = long, latitude = lat)\n",
    "            fecm_int = fecm_cell.interpolate_na(dim = 'forecast_time')\n",
    "            # hobs_int['cl'] = target_cl\n",
    "\n",
    "            # operations\n",
    "            # x inputs\n",
    "\n",
    "            y = class_forecast.isel(lead_time = 0).sel(longitude = long, latitude = lat)\n",
    "            yy = y.to_dataframe().reset_index()\n",
    "            yy_inp = yy.drop(['latitude','longitude', 'lead_time'], axis = 'columns') # valid_time is not found in the case of using target_class\n",
    "            yy_inpVar = yy_inp[['tp']]\n",
    "\n",
    "            y_test = yy_inpVar.values.ravel()\n",
    "\n",
    "            m = 1\n",
    "            for r in range(0,11): \n",
    "\n",
    "                # training and testing data\n",
    "                x = fecm_int.isel(realization = m)\n",
    "                xx = x.to_dataframe().reset_index()\n",
    "                xx['tp_lag1'] = xx.tp.shift(1).bfill()\n",
    "\n",
    "                if spatial == 'one':\n",
    "                    disla = lat - (1 * 1.5)\n",
    "                    dislo = long - (1 * 1.5)\n",
    "                    dla = np.arange(0, 4.5, 1.5)\n",
    "                    dlo = np.arange(0, 4.5, 1.5)\n",
    "                    v = 'var 4'\n",
    "\n",
    "                if spatial == 'two':\n",
    "                    disla = lat - (2 * 1.5)\n",
    "                    dislo = long - (2 * 1.5)\n",
    "                    dla = np.arange(0, 7.5, 1.5)\n",
    "                    dlo = np.arange(0, 7.5, 1.5)\n",
    "                    v = 'var 12'\n",
    "\n",
    "                if spatial == 'three':\n",
    "                    disla = lat - (3 * 1.5)\n",
    "                    dislo = long - (3 * 1.5)\n",
    "                    dla = np.arange(0, 10.5, 1.5)\n",
    "                    dlo = np.arange(0, 10.5, 1.5)\n",
    "                    v = 'var 24'\n",
    "\n",
    "                # var = []\n",
    "                nam = 0\n",
    "                for ii in dlo:\n",
    "                    for jj in dla:\n",
    "\n",
    "                        xx['Var'] = fecm_interpolate.tp.sel(latitude = disla + jj, longitude = dislo + ii).isel(realization = m)\n",
    "                        name = 'var {}'.format(nam)\n",
    "                        xx.rename(columns={'Var': name}, inplace=True)\n",
    "                        nam = nam + 1          \n",
    "\n",
    "                xx_inp = xx.drop(['forecast_time','latitude','longitude', 'lead_time', 'valid_time', 'realization'], axis = 'columns')\n",
    "                xx_inpVar = xx_inp.drop([v], axis = 'columns')\n",
    "\n",
    "                x_test = xx_inpVar.values\n",
    "\n",
    "                # Training the LR and MLP models\n",
    "\n",
    "                min_max_scaler = preprocessing.MinMaxScaler()\n",
    "                x_testT = min_max_scaler.fit_transform(x_test)\n",
    "\n",
    "                m = m + 1\n",
    "    #             if r == 9:\n",
    "    #                 m = 50\n",
    "\n",
    "                if  model == 'LR':\n",
    "                    lr = LogisticRegression()\n",
    "                    lr_model = lr.fit(x_trainT, y_train)\n",
    "                    training_accuracy = lr_model.score(x_train, y_train)\n",
    "    #                 print(training_accuracy)\n",
    "\n",
    "                if model == 'MLP':\n",
    "\n",
    "                    # load the model\n",
    "                    Model = load_model('models/'+ region + spatial +'/model_' + str(lead_time) + str(r + 1) + '_' +  str(i) + str(j) + '.h5')\n",
    "\n",
    "                    # make prediction\n",
    "                    yhat = Model.predict(x_testT, verbose=0)\n",
    "\n",
    "                    # stack predictions into [rows, members, probabilities]\n",
    "                    if stackX is None:\n",
    "                        stackX = yhat\n",
    "                    else:\n",
    "                        stackX = dstack((stackX, yhat))\n",
    "\n",
    "            stackXX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "            stackXX = np.nan_to_num(stackXX)\n",
    "            # Load the meta model\n",
    "\n",
    "            meta_model = pickle.load(open('models/' + region + spatial + '/meta_model_' + str(lead_time) + '_' +  str(i) + str(j) + '.sav', 'rb'))\n",
    "\n",
    "            yy_pred = meta_model.predict_proba(stackXX)\n",
    "\n",
    "            # add the values to the empty array\n",
    "\n",
    "            prediction_an[:, j, i] = yy_pred[:,2]\n",
    "            prediction_nn[:, j, i] = yy_pred[:,1]\n",
    "            prediction_bn[:, j, i] = yy_pred[:,0]\n",
    "\n",
    "            j = j + 1 \n",
    "\n",
    "            print(1)\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    # Make an xarray of predictions data\n",
    "\n",
    "    time_step = [i for i in range(0, len(yy_pred))]\n",
    "\n",
    "    longitude = longitude.values\n",
    "    latitude = latitude.values\n",
    "\n",
    "\n",
    "    predict_an = xr.DataArray(\n",
    "    data=prediction_an,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "    longitude=([\"longitude\"], longitude),\n",
    "    latitude=([\"latitude\"], latitude),      \n",
    "    time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "    description=\"predictions_an\",\n",
    "    ),\n",
    "    )\n",
    "\n",
    "    predict_nn = xr.DataArray(\n",
    "    data=prediction_nn,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "    longitude=([\"longitude\"], longitude),\n",
    "    latitude=([\"latitude\"], latitude),      \n",
    "    time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "    description=\"predictions_nn\",\n",
    "    ),\n",
    "    )\n",
    "\n",
    "    predict_bn = xr.DataArray(\n",
    "    data=prediction_bn,\n",
    "    dims=[\"time_step\",\"latitude\", \"longitude\"],\n",
    "    coords=dict(\n",
    "    longitude=([\"longitude\"], longitude),\n",
    "    latitude=([\"latitude\"], latitude),      \n",
    "    time_step= ([\"time_step\"], time_step)\n",
    "    ),\n",
    "    attrs=dict(\n",
    "    description=\"predictions_bn\",\n",
    "    ),\n",
    "    )\n",
    "\n",
    "    # make categories from the predictions\n",
    "\n",
    "    categories = xr.concat([predict_bn, predict_nn, predict_an],'category').assign_coords(category=['below normal', 'near normal', 'above normal'])\n",
    "    categorical_p = categories.rename({'time_step':'forecast_time'})\n",
    "    \n",
    "    return categorical_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_p = CMte2020cor('region1', 'one' ,'MLP', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_p['forecast_time'] = obs_p['forecast_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RPSS_cl = RPSS_2020('region3', categorical_p, '2020-01-02', '2020-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region1 = ROC_ML2020(forecast_observation , categorical_p, tercile_edges, 100, 36, 'tp', 0, '2020-01-02', '2020-12-31')\n",
    "# region2 = ROC_ML2020(forecast_observation , categorical_p, tercile_edges, 100, 55, 'tp', 0, '2020-01-02', '2020-12-31')\n",
    "NNl_roc3 = ROC_ML2020(forecast_observation , categorical_p, tercile_edges, 6, 53, 'tp', 0, '2020-01-02', '2020-12-31')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
